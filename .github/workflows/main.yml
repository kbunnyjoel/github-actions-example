name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Set to true to destroy the EKS cluster'
        required: false
        default: 'false'
      run_terraform:
        description: 'Run Terraform? (true/false)'
        required: false
        default: 'true'
  push:
    branches: [ main ]
    paths:
      - 'apps/**'
      - 'manifests/templates/**'
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/${{ secrets.DOCKER_USERNAME }}

jobs:
  buil-and-test:
    permissions:
      actions: read # Default permission for GITHUB_TOKEN
      contents: read # To checkout the repository
      security-events: write # Required for CodeQL to upload SARIF results
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:javascript"

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: buil-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/${{ env.APP_NAME }}:${{ github.sha }}

      # Scan the image after it has been built and loaded/pushed
      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}'
          format: 'table'
          exit-code: '1' # Fail the build if vulnerabilities are found (adjust severity as needed)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          # Ensure the scanner checks the local Docker daemon
          # Trivy action usually does this by default if image is found locally.

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}

      - name: Wait for app to be ready
        run: |
          for i in {1..20}; do
            curl --fail http://localhost:3000/status && exit 0
            echo "Waiting for app to be ready ($i/20)..."
            sleep 5
          done

  terraform-eks:
    if: ${{ github.event.inputs.run_terraform == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.11.0 # Updated to a more recent stable version

      - name: Run tfsec IaC scan
        uses: aquasecurity/tfsec-action@v1.0.0 # Use a specific version if preferred
        with:
          working_directory: terraform
          addtional_args: '--format json --output-file tfsec-report.json'
          soft_fail: true # Uncomment to see warnings without failing the build initially
      
      - name: Upload tfsec report
        uses: actions/upload-artifact@v4
        with:
          name: tfsec-report
          path: terraform/tfsec-report.json
      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        run: terraform validate
      - name: Terraform Format
        run: terraform fmt -check
      
      - name: Terraform Plan
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform
        if: github.event.inputs.destroy != 'true'
        run: terraform apply -auto-approve

      - name: Generate kubeconfig and upload
        if: github.event.inputs.destroy != 'true'
        run: |
          mkdir -p /tmp/kubeconfig
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example --kubeconfig /tmp/kubeconfig/config
        env:
          AWS_REGION: ap-southeast-2
        shell: bash

      - name: Upload kubeconfig as artifact
        uses: actions/upload-artifact@v4
        with:
          name: bastion-kubeconfig
          path: /tmp/kubeconfig/config

      - name: Upload deployment_key.pem as artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem
          
      - name: Update EKS security group for GitHub Actions
        run: |
          # Get the EKS cluster security group ID
          CLUSTER_SG=$(aws eks describe-cluster --name github-actions-eks-example --query "cluster.resourcesVpcConfig.clusterSecurityGroupId" --output text)
          echo "Cluster security group: $CLUSTER_SG"
          
          # Add rules using JSON format
          aws ec2 authorize-security-group-ingress \
            --group-id $CLUSTER_SG \
            --ip-permissions '[{"IpProtocol": "tcp", "FromPort": 443, "ToPort": 443, "IpRanges": [{"CidrIp": "140.82.112.0/20", "Description": "Allow GitHub Actions"}]}]'
          
          aws ec2 authorize-security-group-ingress \
            --group-id $CLUSTER_SG \
            --ip-permissions '[{"IpProtocol": "tcp", "FromPort": 443, "ToPort": 443, "IpRanges": [{"CidrIp": "185.199.108.0/22", "Description": "Allow GitHub Actions"}]}]'
        continue-on-error: true  # In case rules already exist

      - name: Install kubectl and check EKS status
        if: github.event.inputs.destroy != 'true'
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes --request-timeout=60s
          echo "Checking component statuses..."
          kubectl get componentstatuses --request-timeout=60s
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system --request-timeout=60s

  
  destroy-infrastructure:
    if: github.event.inputs.destroy == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install required tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example || true
      - name: Clean up Kubernetes resources
        run: |
          echo "Cleaning up Kubernetes resources (Ingresses, Services, Deployments, Namespaces)..."
          kubectl delete ingress --all --all-namespaces || true

          # Delete services of type LoadBalancer first and wait for AWS LBs to be de-provisioned
          echo "Deleting LoadBalancer services in argocd namespace..."
          kubectl delete service -n argocd --selector='service.kubernetes.io/load-balancer-cleanup' || kubectl delete service --all -n argocd --field-selector type=LoadBalancer || true
          echo "Deleting LoadBalancer services in ingress-nginx namespace..."
          kubectl delete service -n ingress-nginx --selector='service.kubernetes.io/load-balancer-cleanup' || kubectl delete service --all -n ingress-nginx --field-selector type=LoadBalancer || true
          
          echo "Waiting up to 5 minutes for AWS Load Balancers from Kubernetes services to be deleted..."
          # This is a best-effort wait; more sophisticated polling might be needed for very complex setups
          # The goal is to give AWS time to react to the k8s service deletions.
          sleep 300 

          kubectl delete deployment --all -n argocd || true
          kubectl delete deployment --all -n ingress-nginx || true
          # Delete other resources within namespaces before deleting namespaces
          kubectl delete all --all -n argocd || true
          kubectl delete all --all -n ingress-nginx || true
          kubectl delete namespace argocd || true
          kubectl delete namespace ingress-nginx || true

      - name: Clean up AWS resources
        run: |
          echo "Cleaning up AWS resources..."
          
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query "Vpcs[0].VpcId" --output text || echo "")
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" == "None" ]; then
            echo "VPC not found, skipping cleanup"
            exit 0
          fi
          echo "VPC ID: $VPC_ID"
          
          # Delete load balancers within the specific VPC
          echo "Deleting AWS Load Balancers in VPC $VPC_ID..."
          aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | tr '\t' '\n' | while read LB_ARN; do
            if [ ! -z "$LB_ARN" ]; then
              echo "Deleting load balancer $LB_ARN..."
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            fi
          done
          
          # Wait for load balancers to be deleted
          echo "Waiting for AWS Load Balancers to be fully deleted..."
          sleep 120 # Increased wait time
          
          # Release Elastic IPs associated with the VPC (e.g., NAT Gateways if not managed by TF, or orphaned)
          # WARNING: This can be risky if not properly scoped. Terraform should manage its own EIPs.
          # This example attempts to find EIPs in the VPC that are not associated or associated with NATs.
          echo "Releasing unassociated or NAT Gateway Elastic IPs in VPC $VPC_ID (use with caution)..."
          aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text | tr '\t' '\n' | while read ALLOC_ID; do
            if [ ! -z "$ALLOC_ID" ]; then # Corrected variable from LB to ALLOC_ID
              # Check if EIP is associated with a NAT gateway or unassociated
              ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].AssociationId" --output text)
              # NETWORK_INTERFACE_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].NetworkInterfaceId" --output text) # Not used, can be removed
              IS_NAT_GW_EIP=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" "Name=nat-gateway-address.allocation-id,Values=$ALLOC_ID" --query "NatGateways" --output text)

              if [ "$ASSOCIATION_ID" == "None" ] || [ ! -z "$IS_NAT_GW_EIP" ] ; then
                echo "Attempting to release Elastic IP $ALLOC_ID..."
                # Disassociation might be needed if it's a NAT GW EIP that TF failed to delete
                if [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id "$ASSOCIATION_ID" || echo "Failed to disassociate $ALLOC_ID, might already be disassociated or managed elsewhere."
                    sleep 2
                fi
                aws ec2 release-address --allocation-id "$ALLOC_ID" || echo "Failed to release $ALLOC_ID, it might be in use or managed by Terraform."
              else
                echo "Skipping EIP $ALLOC_ID as it appears to be actively associated with a resource not identified as a NAT GW or unassociated."
              fi
            fi
          done
          
          # Get all subnets in the VPC
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text)
          echo "Subnets: $SUBNETS"

          # Delete network interfaces in each subnet
          for SUBNET in $SUBNETS; do
            echo "Cleaning up resources in subnet $SUBNET..."
            aws ec2 describe-network-interfaces --filters "Name=subnet-id,Values=$SUBNET" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text | tr '\t' '\n' | while read NI; do
              if [ ! -z "$NI" ]; then
                echo "Checking network interface $NI..."
                ATTACHMENT=$(aws ec2 describe-network-interfaces --network-interface-ids $NI --query "NetworkInterfaces[0].Attachment.AttachmentId" --output text)
                if [ "$ATTACHMENT" != "None" ] && [ ! -z "$ATTACHMENT" ]; then
                  echo "Detaching network interface $NI (attachment $ATTACHMENT)..."
                  aws ec2 detach-network-interface --attachment-id "$ATTACHMENT" --force
                  sleep 5
                fi
                echo "Deleting network interface $NI..."
                aws ec2 delete-network-interface --network-interface-id "$NI"
              fi
            done
          done
          
          # Wait for resources to be released
          echo "Waiting for resources to be released..."
          sleep 120 # Increased wait time

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.11.0 # Updated to a more recent stable version

      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Destroy
        working-directory: terraform
        run: terraform destroy -auto-approve

      
  deploy-argocd:
    if: github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true'
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Add Helm repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Install NGINX Ingress Controller
        run: |
          echo "Installing NGINX Ingress Controller for AWS..."
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.replicaCount=2 \
            --version 4.10.1 \
            --wait
          echo "Waiting for ingress-nginx controller pod to be ready..."
          kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Install Argo CD
        run: |
          echo "Installing ArgoCD..."
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd --create-namespace \
            --set server.ingress.enabled=true \
            --set server.ingress.ingressClassName=nginx \
            --set server.ingress.hostname=argocd.bunnycloud.xyz \
            --set server.ingress.annotations."external-dns\.alpha\.kubernetes\.io/hostname"=argocd.bunnycloud.xyz. \
            --set server.ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
            --version 6.11.1 \
            --wait
          echo "Checking ArgoCD pods status..."
          kubectl get pods -n argocd
          echo "Waiting for ArgoCD deployments to be ready..."
          kubectl wait --for=condition=Available deployment --all -n argocd --timeout=5m
          echo "ArgoCD installed and deployments are ready."

      - name: Install ExternalDNS
        run: |
          echo "Installing ExternalDNS..."
          # Replace with your actual Hosted Zone ID and domain filter
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace external-dns --create-namespace \
            --set provider=aws \
            --set aws.zoneType=public \
            --set domainFilters[0]=bunnycloud.xyz \
            --set policy=upsert-only \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.EXTERNAL_DNS_IAM_ROLE_NAME }}" \
            --set txtOwnerId=$HOSTED_ZONE_ID \
            --version 1.14.5 \
            --wait
          echo "ExternalDNS installed."

      - name: Verify Ingress and DNS (ArgoCD)
        run: |
          echo "Waiting for ArgoCD Ingress or Service to get an address..."
          HOSTNAME=""
          for i in {1..30}; do
            # Check for Ingress
            INGRESS_HOSTNAME=$(kubectl get ingress -n argocd -l app.kubernetes.io/name=argocd-server -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            # If Ingress not found, check for LoadBalancer Service
            if [ -z "$INGRESS_HOSTNAME" ]; then
              INGRESS_HOSTNAME=$(kubectl get svc -n argocd argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            fi
            
            if [ ! -z "$INGRESS_HOSTNAME" ]; then
              echo "LoadBalancer hostname found: $INGRESS_HOSTNAME"
              break
            fi
            echo "Waiting for LoadBalancer... (attempt $i/30)"
            sleep 20
          done
          
          if [ -z "$INGRESS_HOSTNAME" ]; then
            echo "Error: ArgoCD LoadBalancer not found after timeout."
            echo "Checking Ingress resources:"
            kubectl get ingress -n argocd -o wide
            echo "Checking Service resources:"
            kubectl get svc -n argocd -o wide
            exit 1
          fi
          
          echo "Waiting for DNS record for argocd.bunnycloud.xyz to be created by ExternalDNS..."
          for i in {1..20}; do # Wait up to 10 minutes
            if dig +short argocd.bunnycloud.xyz | grep -q .; then
              echo "DNS record found for argocd.bunnycloud.xyz"
              break
            fi
            echo "DNS not propagated yet for argocd.bunnycloud.xyz... ($i/20)"
            sleep 30
          done
    dig argocd.bunnycloud.xyz # Final check

      - name: Check and update EKS node security group
        run: |
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Checking if security group allows inbound traffic on port 8080..."
          
          # Check if the rule exists
          RULE_EXISTS=$(aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`8080\` && ToPort==\`8080\` && IpProtocol==\`tcp\`]" --output text)
          
          # Note: This rule for port 8080 on nodes might be for a direct LoadBalancer service.
          # If using an Ingress controller (like Nginx or AWS LBC), the relevant ports would be
          # the NodePorts used by the Ingress controller service, and the ELB it creates should have access to them.
          
      # The 'Check domain nameserver delegation' and 'Check Route53 record' steps can remain for verification.
      # The 'Update Nodejs App DNS record' step is removed as ExternalDNS will handle it
      # based on annotations in your Node.js app's Ingress (defined in its Helm chart).

      - name: Verify Node.js App DNS (assuming its Ingress is annotated for external-dns)
        run: |
          echo "Waiting for DNS record for nodejs.bunnycloud.xyz to be created by ExternalDNS..."
          for i in {1..20}; do # Wait up to 10 minutes
            dig +short nodejs.bunnycloud.xyz && break
            echo "DNS not propagated yet for nodejs.bunnycloud.xyz... ($i/20)"
            sleep 30
          done
          dig nodejs.bunnycloud.xyz # Final check

      - name: Old Test DNS record (can be removed or adapted) # Renamed this step
        run: |
          echo "Testing DNS record for argocd.bunnycloud.xyz..."
          dig argocd.bunnycloud.xyz
          
          echo "Waiting for DNS propagation..."
          for i in {1..10}; do
            IP=$(dig +short argocd.bunnycloud.xyz)
            if [ ! -z "$IP" ]; then
              echo "DNS record found: argocd.bunnycloud.xyz -> $IP"
              break
            fi
            echo "Waiting for DNS propagation... (attempt $i/10)"
            sleep 30
          done

  generate-argo-apps:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python and install Jinja2
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install jinja2-cli
      # Ensure yq is installed if you need it for other YAML processing
      # sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq
      - name: Detect new app folders
        id: detect
        run: |
          echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
      - name: Generate manifests
        run: |
          mkdir -p k8s/argocd/applications
          APPS_JSON='${{ steps.detect.outputs.apps }}'
    
          echo "$APPS_JSON" | jq -r '.[]' | while read app; do
            OUTFILE="k8s/argocd/applications/${app}.yaml"
            echo "Rendering $OUTFILE..."
    
            # Define image repository and tag
            # Use app folder name as image name
            APP_SPECIFIC_IMAGE_NAME="${app}" # Use app folder name as image name
            IMAGE_REPOSITORY="${{ secrets.DOCKER_USERNAME }}/${APP_SPECIFIC_IMAGE_NAME}"
            IMAGE_TAG="${{ github.sha }}"
            TARGET_REVISION_BRANCH="main" # Or derive from github.ref
            DEPLOYMENT_NAMESPACE="$app" # Deploy each app to its own namespace, e.g., 'nodejs'
    
            jinja2 manifests/templates/argocd-app.yaml.j2 \
              -D app_name="$app" -D chart_path="apps/$app/chart" \
              -D github_repository_url="${{ github.server_url }}/${{ github.repository }}" \
              -D target_revision="$TARGET_REVISION_BRANCH" \
              -D image_repository="$IMAGE_REPOSITORY" \
              -D image_tag="$IMAGE_TAG" \
              -D deployment_namespace="$DEPLOYMENT_NAMESPACE" \
              > "$OUTFILE"
            echo "Generated manifest for $app:"
            cat "$OUTFILE"
          done
      - name: Commit generated ArgoCD Application manifests
        env:
          # Use PAT if GITHUB_TOKEN doesn't have push rights to main or if main is protected
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }} # Assuming this PAT has repo write access
        run: |
          # Commits directly to the branch the workflow is running on (e.g., 'main' after a PR merge).
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add k8s/argocd/applications/
          # Commit and push only if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "Generate/Update ArgoCD Application manifests for ${{ github.sha }}"
            git push origin HEAD:${{ github.ref_name }} # Push to the current branch
            echo "Committed and pushed ArgoCD Application manifests to ${{ github.ref_name }}."
          else
            echo "No changes to ArgoCD Application manifests to commit."
          fi

  lint-and-deploy-helm:
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    needs: [docker, deploy-argocd] # Changed from generate-argo-apps to deploy-argocd
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      - name: Lint Helm charts
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Linting $chart_dir/chart..."
                helm lint "$chart_dir/chart"
              fi
            done
          fi
      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Rendering $chart_dir/chart..."
                # You might want to pass values files or set values here for a more realistic render
                helm template "$chart_dir/chart"
              fi
            done
          fi
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

      - name: Apply ArgoCD Application manifests
        run: |
          # Apply all generated application manifests
          if [ -d "k8s/argocd/applications" ]; then
            for app_manifest in k8s/argocd/applications/*.yaml; do
              if [ -f "$app_manifest" ]; then
                echo "Applying ArgoCD Application manifest: $app_manifest"
                kubectl apply -f "$app_manifest"
              fi
            done
          else
            echo "No ArgoCD application manifests found in k8s/argocd/applications."
          fi

      - name: Wait for Node.js app to be ready (Example for 'nodejs' app)
        # This step needs to be generalized or made conditional if you have multiple apps
        # For now, it assumes a 'nodejs' app as per previous logic.
        run: |
          APP_NAMESPACE="nodejs" # Assuming the app is deployed to a namespace matching its name
          APP_SERVICE_NAME="nodejs-service" # Assuming this is the service name for the nodejs app

          echo "Waiting for $APP_NAMESPACE pod to be ready..."
          for i in {1..30}; do
            # Adjust label selector based on your Helm chart's conventions
            kubectl get pods -n $APP_NAMESPACE -l app.kubernetes.io/name=$APP_NAMESPACE --field-selector=status.phase=Running && break
            sleep 10
          done

          echo "Port-forwarding service $APP_SERVICE_NAME and checking health endpoint..."
          kubectl port-forward svc/$APP_SERVICE_NAME -n $APP_NAMESPACE 8080:80 & # Forward to containerPort (e.g. 3000), service port is 80
          PORT_FORWARD_PID=$!
          sleep 5 # Give port-forward time to establish

          HEALTHY=false
          for i in {1..10}; do
            # Assuming your app's /status endpoint is on the containerPort (e.g., 3000)
            # and the service port-forwards 80 to that containerPort.
            # If your app listens on 3000 and service is 80->3000, then curl to localhost:8080/status
            # If your app listens on 80 and service is 80->80, then curl to localhost:8080/status
            # Adjust the port in curl command if your app's actual listening port (containerPort) is different
            # and your service definition. The example assumes service port 80 maps to a container port.
            # If your nodejs app listens on 3000, and service maps 80->3000, then port-forward 8080:80 (servicePort)
            # and curl localhost:8080/status.
            # If your nodejs app listens on 3000, and service maps 3000->3000, then port-forward 8080:3000
            # and curl localhost:8080/status.
            # The current port-forward is 8080 (local) to 80 (service's targetPort, assuming it's the app's listening port).
            # If your app listens on 3000, the service should targetPort 3000.
            # And the port-forward should be to the service's port (e.g. 80) or targetPort (e.g. 3000)
            # Let's assume the service is configured correctly and the app listens on the targetPort of the service's port 80.
            if curl --fail --max-time 10 http://localhost:8080/status; then
              echo "✅ App $APP_NAMESPACE is healthy"
              HEALTHY=true
              break
            fi
            echo "Waiting for app $APP_NAMESPACE to respond... ($i/10)"
            sleep 5
          done

          kill $PORT_FORWARD_PID # Kill the port-forward process

          if [ "$HEALTHY" = "false" ]; then
            echo "❌ App $APP_NAMESPACE did not become healthy in time"
            exit 1
          fi
