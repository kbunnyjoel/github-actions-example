name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Set to true to destroy the EKS cluster'
        required: false
        default: 'false'
      run_terraform:
        description: 'Run Terraform? (true/false)'
        required: false
        default: 'true'
  push:
    branches: [ main ]
    paths:
      - 'apps/**'
      - 'manifests/templates/**'
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/${{ secrets.DOCKER_USERNAME }}

jobs:

  build-and-test:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    permissions:
      actions: read # Default permission for GITHUB_TOKEN
      contents: read # To checkout the repository
      security-events: write # Required for CodeQL to upload SARIF results
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:javascript"

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: build-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/${{ env.APP_NAME }}:${{ github.sha }}

      # Scan the image after it has been built and loaded/pushed
      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}'
          format: 'table'
          exit-code: '1' # Fail the build if vulnerabilities are found (adjust severity as needed)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          # Ensure the scanner checks the local Docker daemon
          # Trivy action usually does this by default if image is found locally.

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}

      - name: Wait for app to be ready
        run: |
          for i in {1..20}; do
            curl --fail http://localhost:3000/status && exit 0
            echo "Waiting for app to be ready ($i/20)..."
            sleep 5
          done

  terraform-eks:
    if: ${{ github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Run tfsec IaC scan
        uses: aquasecurity/tfsec-action@v1.0.0 # Use a specific version if preferred
        with:
          working_directory: terraform
          addtional_args: '--format json --output-file tfsec-report.json'
          soft_fail: true # Uncomment to see warnings without failing the build initially
      
      - name: Upload tfsec report
        uses: actions/upload-artifact@v4
        with:
          name: tfsec-report
          path: terraform/tfsec-report.json
      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        run: terraform validate
      - name: Terraform Format
        run: terraform fmt -check
      
      - name: Terraform Unlock
        run: terraform force-unlock -force 1bcecd76-352b-7fd1-1dce-442e67ecb6c8 || echo "No locks to unlock"
      - name: Terraform Plan
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform
        if: env.IS_DESTROY != 'true'
        run: terraform apply -auto-approve

      - name: Generate kubeconfig and upload
        if: env.IS_DESTROY != 'true'
        run: |
          mkdir -p /tmp/kubeconfig
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example --kubeconfig /tmp/kubeconfig/config
        env:
          AWS_REGION: ap-southeast-2
        shell: bash

      - name: Upload kubeconfig as artifact
        uses: actions/upload-artifact@v4
        with:
          name: bastion-kubeconfig
          path: /tmp/kubeconfig/config

      - name: Upload deployment_key.pem as artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem

      - name: Install kubectl and check EKS status
        if: env.IS_DESTROY != 'true'
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes --request-timeout=60s
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system --request-timeout=60s

  
  destroy-infrastructure:
    if: ${{ github.event.inputs.destroy == 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install required tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example || true
      - name: Clean up AWS resources
        run: |
          echo "Cleaning up AWS resources..."
          
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query "Vpcs[0].VpcId" --output text || echo "")
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" == "None" ]; then
            echo "VPC not found, skipping cleanup"
            exit 0
          fi
          echo "VPC ID: $VPC_ID"
          
          # Delete load balancers within the specific VPC
          echo "Deleting AWS Load Balancers in VPC $VPC_ID..."
          aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | tr '\t' '\n' | while read LB_ARN; do
            if [ ! -z "$LB_ARN" ]; then
              echo "Deleting load balancer $LB_ARN..."
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            fi
          done
          
          # Wait for load balancers to be deleted
          echo "Waiting for AWS Load Balancers to be fully deleted..."
          sleep 120 # Increased wait time
          
          # Release Elastic IPs associated with the VPC (e.g., NAT Gateways if not managed by TF, or orphaned)
          # WARNING: This can be risky if not properly scoped. Terraform should manage its own EIPs.
          # This example attempts to find EIPs in the VPC that are not associated or associated with NATs.
          echo "Releasing unassociated or NAT Gateway Elastic IPs in VPC $VPC_ID (use with caution)..."
          aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text | tr '\t' '\n' | while read ALLOC_ID; do
            if [ ! -z "$ALLOC_ID" ]; then # Corrected variable from LB to ALLOC_ID
              # Check if EIP is associated with a NAT gateway or unassociated
              ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].AssociationId" --output text)
              # NETWORK_INTERFACE_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].NetworkInterfaceId" --output text) # Not used, can be removed
              IS_NAT_GW_EIP=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" "Name=nat-gateway-address.allocation-id,Values=$ALLOC_ID" --query "NatGateways" --output text)

              if [ "$ASSOCIATION_ID" == "None" ] || [ ! -z "$IS_NAT_GW_EIP" ] ; then
                echo "Attempting to release Elastic IP $ALLOC_ID..."
                # Disassociation might be needed if it's a NAT GW EIP that TF failed to delete
                if [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id "$ASSOCIATION_ID" || echo "Failed to disassociate $ALLOC_ID, might already be disassociated or managed elsewhere."
                    sleep 2
                fi
                aws ec2 release-address --allocation-id "$ALLOC_ID" || echo "Failed to release $ALLOC_ID, it might be in use or managed by Terraform."
              else
                echo "Skipping EIP $ALLOC_ID as it appears to be actively associated with a resource not identified as a NAT GW or unassociated."
              fi
            fi
          done
          
          # Get all subnets in the VPC
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text)
          echo "Subnets: $SUBNETS"
          echo "Note: Terraform destroy should handle the deletion of VPC-specific resources like NAT Gateways, EIPs, and Network Interfaces created by Terraform."
          
          # Wait for resources to be released
          echo "Waiting for resources to be released..."
          sleep 120 # Increased wait time

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Destroy
        working-directory: terraform
        run: terraform destroy -auto-approve

      
  deploy-argocd:
    if: github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true'
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Init Terraform
        working-directory: terraform
        run: terraform init

      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Add Helm repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Get ACM certificate ARN from Terraform
        run: |
          # Get the certificate ARN from Terraform output
          cd terraform
          CERT_ARN=$(terraform output -raw acm_certificate_arn || echo "")
          
          if [ -z "$CERT_ARN" ]; then
            echo "Warning: Could not get certificate ARN from Terraform output"
            # Fallback to looking up the certificate
            CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            
            if [ -z "$CERT_ARN" ]; then
              echo "Error: No certificate found for *.bunnycloud.xyz"
              exit 1
            fi
          fi
          
          echo "Using certificate ARN: $CERT_ARN"
          echo "ACM_CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV

      - name: Install NGINX Ingress Controller with SSL
        run: |
          echo "Installing NGINX Ingress Controller for AWS with SSL..."
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${{ env.ACM_CERTIFICATE_ARN }}" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
            --set controller.service.targetPorts.https=http \
            --set controller.service.ports.https=443 \
            --set controller.replicaCount=2 \
            --version 4.10.1 \
            --wait
          
          echo "Waiting for ingress-nginx controller pod to be ready..."
          kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Install ArgoCD
        run: |
          echo "Installing ArgoCD..."
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.ingress.enabled=true \
            --set server.ingress.ingressClassName=nginx \
            --set server.ingress.hostname=argocd.bunnycloud.xyz \
            --set server.ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
            --set server.ingress.annotations."nginx\.ingress\.kubernetes\.io/ssl-redirect"=true \
            --set server.ingress.annotations."nginx\.ingress\.kubernetes\.io/force-ssl-redirect"=true \
            --set server.ingress.annotations."external-dns\.alpha\.kubernetes\.io/hostname"=argocd.bunnycloud.xyz. \
            --set server.ingress.tls[0].hosts[0]=argocd.bunnycloud.xyz \
            --set server.ingress.tls[0].secretName=argocd-tls \
            --version 6.11.1 \
            --wait
          
          echo "Checking ArgoCD pods status..."
          kubectl get pods -n argocd
          echo "Waiting for ArgoCD deployments to be ready..."
          kubectl wait --for=condition=Available deployment --all -n argocd --timeout=5m || true
          echo "ArgoCD installation completed."

      - name: Get ExternalDNS role ARN from Terraform
        run: |
          echo "Getting ExternalDNS role ARN from Terraform..."
          cd terraform

          # Initialize Terraform to access the state
          # The backend configuration in your Terraform files must allow this job to access the state
          echo "Initializing Terraform..."
          if ! terraform init -input=false -reconfigure; then
            echo "Error: Terraform init failed."
            # Decide if you want to exit or attempt fallback. Exiting is safer.
            # exit 1 
          fi

          ROLE_ARN_FROM_TF=$(terraform output -raw external_dns_role_arn 2>/dev/null)

          if [ ! -z "$ROLE_ARN_FROM_TF" ]; then
            echo "Successfully retrieved ExternalDNS role ARN from Terraform output: $ROLE_ARN_FROM_TF"
            ROLE_ARN="$ROLE_ARN_FROM_TF"
          else
            echo "Warning: Could not get ExternalDNS role ARN from Terraform output. Attempting fallback."
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
            if [ -z "$AWS_ACCOUNT_ID" ]; then
              echo "Error: Could not determine AWS Account ID for fallback."
              exit 1
            fi
            # This ROLE_NAME must exactly match the name defined in terraform/external_dns.tf
            FALLBACK_ROLE_NAME="eks-external-dns-role" 
            ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${FALLBACK_ROLE_NAME}"
            echo "Using fallback ExternalDNS role ARN: $ROLE_ARN"
            # Verify the fallback role actually exists
            if ! aws iam get-role --role-name "$FALLBACK_ROLE_NAME" > /dev/null 2>&1; then
                echo "Error: Fallback role '$FALLBACK_ROLE_NAME' does not exist in AWS."
                echo "Please check Terraform apply job and ensure the role was created with this name."
                exit 1
            fi
          fi
          
          echo "Using ExternalDNS role ARN: $ROLE_ARN"
          echo "EXTERNAL_DNS_ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV
          
          # Get hosted zone ID - applying similar robustness
          echo "Getting Hosted Zone ID..."
          HOSTED_ZONE_ID_FROM_TF=$(terraform output -raw route53_zone_id 2>/dev/null)

          if [ ! -z "$HOSTED_ZONE_ID_FROM_TF" ]; then
            echo "Successfully retrieved Hosted Zone ID from Terraform output: $HOSTED_ZONE_ID_FROM_TF"
            HOSTED_ZONE_ID="$HOSTED_ZONE_ID_FROM_TF"
          else
            echo "Warning: Could not get Hosted Zone ID from Terraform output. Attempting fallback."
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          fi

          if [ -z "$ROLE_ARN" ]; then
            echo "Error: ExternalDNS role ARN is empty after all attempts."
            exit 1
          fi
          echo "Using hosted zone ID: $HOSTED_ZONE_ID"
          echo "HOSTED_ZONE_ID=$HOSTED_ZONE_ID" >> $GITHUB_ENV
          
      - name: Install ExternalDNS
        run: |
          echo "Installing ExternalDNS..."
          # Install ExternalDNS
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace external-dns --create-namespace \
            --set sources[0]=service \
            --set sources[1]=ingress \
            --set aws.zoneType=public \
            --set aws.region=ap-southeast-2 \
            --set domainFilters[0]=bunnycloud.xyz \
            # Policy for DNS updates
            --set policy=upsert-only \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.EXTERNAL_DNS_ROLE_ARN }}" \
            # Sets the TXT record owner ID
            --set txtOwnerId="${{ env.HOSTED_ZONE_ID }}" \
            # Ensures CNAMEs are preferred for ELB hostnames
            --set extraArgs[0]=--aws-prefer-cname \
            # Use TXT records for ownership verification
            --set extraArgs[1]=--registry=txt \
            --version 1.14.5 \
            --timeout 10m \
            --wait
          
          echo "ExternalDNS installed."

      - name: Retry ExternalDNS install if deployment fails
        run: |
          echo "Checking if ExternalDNS deployment exists..."
          if ! kubectl rollout status deployment external-dns -n external-dns --timeout=30s; then
            echo "ExternalDNS deployment failed. Reinstalling..."
            helm uninstall external-dns -n external-dns || true
            sleep 10
            kubectl delete secret -n external-dns $(kubectl get secrets -n external-dns | grep sh.helm.release.v1.external-dns | awk '{print $1}') || true
            sleep 5
            helm upgrade --install external-dns external-dns/external-dns \
              --namespace external-dns --create-namespace \
              --set sources[0]=service \
              --set sources[1]=ingress \
              --set aws.zoneType=public \
              --set aws.region=ap-southeast-2 \
              --set domainFilters[0]=bunnycloud.xyz \
              # Policy for DNS updates
              --set policy=upsert-only \
              --set serviceAccount.create=true \
              --set serviceAccount.name=external-dns \
              --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.EXTERNAL_DNS_ROLE_ARN }}" \
              # Sets the TXT record owner ID
              --set txtOwnerId="${{ env.HOSTED_ZONE_ID }}" \
              # Ensures CNAMEs are preferred for ELB hostnames
              --set extraArgs[0]=--aws-prefer-cname \
              # Use TXT records for ownership verification
              --set extraArgs[1]=--registry=txt \
              --version 1.14.5 \
              --timeout 10m \
              --wait
          else
            echo "ExternalDNS is already deployed and healthy."
          fi
          
      - name: Verify ExternalDNS Service Account and Restart Pods
        run: |
          echo "Verifying ExternalDNS service account annotations..."
          kubectl describe sa external-dns -n external-dns
          echo "Restarting ExternalDNS deployment to ensure IRSA is applied..."
          kubectl rollout restart deployment external-dns -n external-dns
          kubectl rollout status deployment external-dns -n external-dns --timeout=2m

          # Check ExternalDNS logs
          echo "Checking ExternalDNS logs..."
          sleep 10  # Wait for ExternalDNS to start
          kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=50 # Consider removing --tail or increasing it for more context if issues persist
        # continue-on-error: true # Recommended to remove this to catch Helm installation errors

      - name: Verify ArgoCD Installation and DNS
        # Changed from manual creation to verification as ExternalDNS handles it
        run: |
          echo "Checking ArgoCD installation status..."
          kubectl get pods -n argocd
          
          echo "Checking ArgoCD service..."
          kubectl get svc -n argocd
          
          echo "Checking ArgoCD ingress..."
          kubectl get ingress -n argocd
          
          echo "Waiting for NGINX Ingress Controller LoadBalancer to get an address..."
          NGINX_LB_HOSTNAME=""
          for i in {1..15}; do
            # Get the NGINX Ingress controller's LoadBalancer hostname
            NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ ! -z "$NGINX_LB_HOSTNAME" ]; then
              echo "NGINX Ingress Controller LoadBalancer hostname found: $NGINX_LB_HOSTNAME"
              break
            fi
            echo "Waiting for NGINX Ingress Controller LoadBalancer hostname... ($i/15)"
            sleep 20
          done
          
          if [ -z "$NGINX_LB_HOSTNAME" ]; then
            echo "Error: Could not find NGINX Ingress Controller LoadBalancer hostname after timeout."
            exit 1 # Fail the step
          fi
          
          echo "Verifying DNS record for argocd.bunnycloud.xyz (should point to $NGINX_LB_HOSTNAME)..."
          echo "Waiting for DNS propagation (max 5 minutes)..."
          for i in {1..10}; do
            # ExternalDNS should create a CNAME record pointing to the NLB hostname.
            # Add a trailing dot to NGINX_LB_HOSTNAME for exact CNAME match, as dig output for CNAME includes it.
            EXPECTED_CNAME_TARGET="${NGINX_LB_HOSTNAME}."
            RESOLVED_CNAME_VALUE=$(dig +short -t CNAME argocd.bunnycloud.xyz @8.8.8.8)

            if [ "$RESOLVED_CNAME_VALUE" = "$EXPECTED_CNAME_TARGET" ]; then
              echo "DNS CNAME record for argocd.bunnycloud.xyz propagated and correctly points to $NGINX_LB_HOSTNAME."
              break
            fi
            echo "Waiting for DNS CNAME propagation for argocd.bunnycloud.xyz... Attempt $i/10. Found: '$RESOLVED_CNAME_VALUE', Expected: '$EXPECTED_CNAME_TARGET'"
            sleep 30
            if [ $i -eq 10 ]; then
              echo "Warning: DNS propagation for argocd.bunnycloud.xyz timed out or record does not point to $NGINX_LB_HOSTNAME."
              echo "Current DNS resolution for argocd.bunnycloud.xyz:"
              echo "Checking CNAME using 8.8.8.8:"
              dig -t CNAME argocd.bunnycloud.xyz @8.8.8.8
              dig argocd.bunnycloud.xyz +trace
            fi
          done
        continue-on-error: true  # Don't fail the workflow if verification fails

      # Security group rules are now managed by Terraform in eks_sg_rules.tf
      - name: Verify EKS security group rules
        run: |
          echo "Verifying EKS security group rules (now managed by Terraform)..."
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Cluster security group: $SG_ID"
          
          # Just verify the rules exist
          echo "Checking if security group allows required inbound traffic..."
          aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`443\` || FromPort==\`8080\`]" --output table
        continue-on-error: true  # Don't fail the workflow if verification fails

  generate-argo-apps:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python and install Jinja2
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install jinja2-cli
      # Ensure yq is installed if you need it for other YAML processing
      # sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq
      - name: Detect new app folders
        id: detect
        run: |
          echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
          
      - name: Generate manifests
        run: |
          mkdir -p k8s/argocd/applications
          APPS_JSON='${{ steps.detect.outputs.apps }}'
          
          # Use repository variables or defaults
          DOCKER_USERNAME="${{ secrets.DOCKER_USERNAME || '1073286' }}"
          DEPLOYMENT_NAMESPACE="${{ vars.DEPLOYMENT_NAMESPACE || 'github-actions-example' }}"
          IMAGE_NAME="github-actions-example"  # Use a consistent image name
          
          # Get ACM certificate ARN
          ACM_CERT_ARN="${{ env.ACM_CERTIFICATE_ARN }}"
          if [ -z "$ACM_CERT_ARN" ]; then
            ACM_CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            echo "Retrieved ACM certificate ARN: $ACM_CERT_ARN"
          fi

          echo "$APPS_JSON" | jq -r '.[]' | while read app; do
            OUTFILE="k8s/argocd/applications/${app}.yaml"
            echo "Rendering $OUTFILE for app: $app"

            # Define image repository and tag
            IMAGE_REPOSITORY="${DOCKER_USERNAME}/${IMAGE_NAME}"  # Use the consistent image name
            IMAGE_TAG="${{ github.sha }}"
            TARGET_REVISION_BRANCH="${{ github.ref_name }}"
            
            # Override app_name to use github-actions-example
            APP_NAME="github-actions-example"

            jinja2 manifests/templates/argocd-app.yaml.j2 \
              -D app_name="$APP_NAME" \
              -D chart_path="apps/$app/chart" \
              -D github_repository_url="${{ github.server_url }}/${{ github.repository }}" \
              -D target_revision="$TARGET_REVISION_BRANCH" \
              -D image_repository="$IMAGE_REPOSITORY" \
              -D image_tag="$IMAGE_TAG" \
              -D deployment_namespace="$DEPLOYMENT_NAMESPACE" \
              -D acm_certificate_arn="$ACM_CERT_ARN" \
              -D elb_hostname="$NGINX_LB_HOSTNAME" \
              > "$OUTFILE"
            echo "Generated manifest for $app (using app_name=$APP_NAME):"
            cat "$OUTFILE"
          done


      - name: Commit generated ArgoCD Application manifests
        env:
          # Use PAT if GITHUB_TOKEN doesn't have push rights to main or if main is protected
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }} # Assuming this PAT has repo write access
        run: |
          # Commits directly to the branch the workflow is running on (e.g., 'main' after a PR merge).
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add k8s/argocd/applications/
          # Commit and push only if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "Generate/Update ArgoCD Application manifests for ${{ github.sha }}"
            git push origin HEAD:${{ github.ref_name }} # Push to the current branch
            echo "Committed and pushed ArgoCD Application manifests to ${{ github.ref_name }}."
          else
            echo "No changes to ArgoCD Application manifests to commit."
          fi

  lint-and-deploy-helm:
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && github.event.inputs.destroy != 'true'
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: [docker, deploy-argocd] # Changed from generate-argo-apps to deploy-argocd
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Lint Helm charts
        run: |
          if [ -d "apps" ]; then  # Changed from "helm" to "apps"
            for chart_dir in apps/*/chart; do
              echo "Linting $chart_dir..."
              helm lint $chart_dir
            done
          else
            echo "No apps directory found, skipping Helm lint"
          fi

      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Rendering $chart_dir/chart..."
                # You might want to pass values files or set values here for a more realistic render
                helm template "$chart_dir/chart"
              fi
            done
          fi
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

      - name: Apply ArgoCD Application manifests
        run: |
          # Apply all generated application manifests
          if [ -d "k8s/argocd/applications" ]; then
            for app_manifest in k8s/argocd/applications/*.yaml; do
              if [ -f "$app_manifest" ]; then
                echo "Applying ArgoCD Application manifest: $app_manifest"
                kubectl apply -f "$app_manifest"
              fi
            done
          else
            echo "No ArgoCD application manifests found in k8s/argocd/applications."
          fi

      - name: Wait for Node.js app to be ready
        run: |
          APP_NAMESPACE="github-actions-example"
          APP_NAME="github-actions-example"  # Use the consistent app name

          echo "Creating namespace $APP_NAMESPACE if it doesn't exist..."
          kubectl create namespace $APP_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          echo "Waiting for ArgoCD to deploy the application..."
          # Wait for ArgoCD to create resources (up to 5 minutes)
          for i in {1..30}; do
            if kubectl get deployment -n $APP_NAMESPACE 2>/dev/null | grep -q "$APP_NAME"; then
              echo "Deployment found in namespace $APP_NAMESPACE"
              break
            fi
            echo "Waiting for deployment to be created... ($i/30)"
            sleep 10
            if [ $i -eq 30 ]; then
              echo "Warning: Deployment not found after timeout. ArgoCD may still be processing."
              echo "Checking ArgoCD application status:"
              kubectl get applications -n argocd
              echo "Continuing anyway..."
            fi
          done

          echo "Waiting for $APP_NAMESPACE pods to be ready..."
          kubectl get pods -n $APP_NAMESPACE
        continue-on-error: true  # Don't fail the workflow if the app isn't ready yet

      - name: Verify DNS record for application (managed by ExternalDNS)
        run: |
          APP_TARGET_HOSTNAME="github-actions-example.bunnycloud.xyz"
          echo "Verifying DNS record for $APP_TARGET_HOSTNAME..."
          
          # Get the NGINX Ingress Controller's LoadBalancer hostname, which is the CNAME target
          NGINX_LB_HOSTNAME=""
          for i in {1..15}; do
            NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ ! -z "$NGINX_LB_HOSTNAME" ]; then
              echo "NGINX Ingress Controller LoadBalancer hostname found: $NGINX_LB_HOSTNAME"
              break
            fi
            echo "Waiting for NGINX Ingress Controller LoadBalancer hostname... ($i/15)"
            sleep 20
          done
          
          if [ -z "$NGINX_LB_HOSTNAME" ]; then
            echo "Error: Could not find NGINX Ingress Controller LoadBalancer hostname after timeout."
            echo "Debug: Listing services in ingress-nginx namespace:"
            kubectl get svc -n ingress-nginx
            exit 1 # Fail the step
          fi
          
          echo "ExternalDNS should create a CNAME record for $APP_TARGET_HOSTNAME pointing to $NGINX_LB_HOSTNAME."
          echo "Waiting for DNS propagation (max 5 minutes)..."
          for i in {1..10}; do
            # Check if the CNAME resolves and points to the correct target
            # ExternalDNS should create a CNAME record pointing to the NLB hostname.
            # Add a trailing dot to NGINX_LB_HOSTNAME for exact CNAME match, as dig output for CNAME includes it.
            EXPECTED_CNAME_TARGET="${NGINX_LB_HOSTNAME}."
            RESOLVED_CNAME_VALUE=$(dig +short -t CNAME $APP_TARGET_HOSTNAME @8.8.8.8)

            if [ "$RESOLVED_CNAME_VALUE" = "$EXPECTED_CNAME_TARGET" ]; then
              echo "DNS CNAME record for $APP_TARGET_HOSTNAME propagated and correctly points to $NGINX_LB_HOSTNAME."
              break
            fi
            echo "Waiting for DNS CNAME propagation for $APP_TARGET_HOSTNAME... Attempt $i/10. Found: '$RESOLVED_CNAME_VALUE', Expected: '$EXPECTED_CNAME_TARGET'"
            sleep 30
            if [ $i -eq 10 ]; then
              echo "Error: DNS propagation for $APP_TARGET_HOSTNAME timed out or record does not point to $NGINX_LB_HOSTNAME."
              echo "Current DNS resolution for $APP_TARGET_HOSTNAME:"
              dig $APP_TARGET_HOSTNAME +trace
              echo "Ensure the application's Ingress resource in namespace 'github-actions-example' has the annotation 'external-dns.alpha.kubernetes.io/hostname: $APP_TARGET_HOSTNAME.' and that ExternalDNS is running correctly."
              echo "Listing Ingresses in github-actions-example namespace:"
              kubectl get ingress -n github-actions-example -o yaml
              echo "ExternalDNS logs:"
              kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=100
              exit 1 # Fail the step
            fi
          done
        # continue-on-error: true # Ensure this is removed or false to make DNS verification failure explicit

      - name: Verify SSL/TLS for Node.js application
        run: |
          echo "Verifying SSL/TLS encryption for Node.js application..."
          
          # Get the application URL
          APP_URL="https://github-actions-example.bunnycloud.xyz"
          
          # Check if the URL is accessible with HTTPS
          echo "Checking if $APP_URL is accessible with HTTPS..."
          if curl -s -o /dev/null -w "%{http_code}" --connect-timeout 30 $APP_URL | grep -q "200\|301\|302"; then
            echo "✅ $APP_URL is accessible with HTTPS"
          else
            echo "❌ $APP_URL is not accessible with HTTPS"
            echo "Checking DNS record..."
            dig github-actions-example.bunnycloud.xyz
          fi
          
          # Verify SSL certificate details
          echo "Verifying SSL certificate details..."
          echo | openssl s_client -servername github-actions-example.bunnycloud.xyz -connect github-actions-example.bunnycloud.xyz:443 2>/dev/null | openssl x509 -noout -text | grep -A2 "Issuer:"
          
          # Check if certificate is from Amazon (ACM)
          if echo | openssl s_client -servername github-actions-example.bunnycloud.xyz -connect github-actions-example.bunnycloud.xyz:443 2>/dev/null | openssl x509 -noout -text | grep -q "Amazon"; then
            echo "✅ Certificate is issued by Amazon (ACM)"
          else
            echo "❌ Certificate is not issued by Amazon"
          fi
        continue-on-error: true  # Don't fail the workflow if verification fails
