name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Set to true to destroy the EKS cluster'
        required: false
        default: 'false'
      run_terraform:
        description: 'Run Terraform? (true/false)'
        required: false
        default: 'true'

  push:
    branches: [ main ]
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/your-dockerhub-username

jobs:
  buil-and-test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: buil-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/my-node-app:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/my-node-app:${{ github.sha }}

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/my-node-app:${{ github.sha }}

      - name: Replace image tag with Git SHA
        run: |
          sed -i "s|__BUILD_SHA__|${{ github.sha }}|g" k8s/deployment/deployment.yaml

      - name: Wait for app to be ready
        run: |
          for i in {1..10}; do
            curl --fail http://localhost:3000/status && exit 0
            sleep 2
          done
          echo "App did not become ready" && exit 1

  terraform-eks:
    if: ${{ github.event.inputs.run_terraform == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.11.0 # Consider updating to a more recent version
      
      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        run: terraform validate
      - name: Terraform Format
        run: terraform fmt -check
      
      - name: Terraform Plan
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform
        if: github.event.inputs.destroy != 'true'
        run: terraform apply -auto-approve
      
      - name: Upload deployment_key.pem as artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem

      - name: Terraform Destroy
        if: github.event.inputs.destroy == 'true'
        run: terraform destroy -auto-approve
          
      - name: Install kubectl and check EKS status
        if: github.event.inputs.destroy != 'true'
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes
          echo "Checking component statuses..."
          kubectl get componentstatuses
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system
      
  deploy-argocd:
    if: github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true'
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Install NGINX Ingress Controller
        run: |
          echo "Installing NGINX Ingress Controller for AWS..."
          kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/aws/deploy.yaml
          echo "Waiting for ingress-nginx controller pod to be ready..."
          kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m


      - name: Install Argo CD
        run: |
          echo "Installing ArgoCD..."
          kubectl create namespace argocd || true
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          
          echo "Checking ArgoCD pods status..."
          kubectl get pods -n argocd
          
          echo "Waiting for ArgoCD deployments to be ready..."
          kubectl wait --for=condition=Available deployment --all -n argocd --timeout=5m
          echo "ArgoCD installed and deployments are ready."

      - name: Apply ArgoCD Ingress
        run: |
          echo "Waiting for Ingress LoadBalancer IP from ingress-nginx-controller..."

          for i in {1..30}; do
            ELB_HOSTNAME=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            if [ ! -z "$ELB_HOSTNAME" ]; then
              echo "Found ELB Hostname: $ELB_HOSTNAME"
              break
            fi
            echo "ELB Hostname not available yet... retrying ($i/30)"
            sleep 20
          done

          if [ -z "$ELB_HOSTNAME" ]; then
            echo "❌ Error: Could not fetch ELB Hostname after timeout."
            kubectl get svc ingress-nginx-controller -n ingress-nginx -o yaml
            exit 1
          fi

          INGRESS_DOMAIN="argocd.bunnycloud.xyz"
          echo "Ingress domain: $INGRESS_DOMAIN"

          cp k8s/argocd/argocd-ingress.yaml k8s/argocd/argocd-ingress-patched.yaml
          yq eval ".spec.rules[0].host = \"${INGRESS_DOMAIN}\"" -i k8s/argocd/argocd-ingress-patched.yaml
          kubectl apply -f k8s/argocd/argocd-ingress-patched.yaml
          echo "✅ Patched Ingress applied successfully."

      - name: Apply ArgoCD LoadBalancer Service
        run: kubectl apply -f k8s/argocd/argocd-svc.yaml

      - name: Wait for Ingress LoadBalancer
        run: |
          echo "Waiting for Ingress LoadBalancer to be provisioned for argocd-server-ingress..."
          INGRESS_HOSTNAME=""
          # Increased timeout as Ingress LBs can take a few minutes
          for i in {1..30}; do
            INGRESS_HOSTNAME=$(kubectl get ingress argocd-server-ingress -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            if [ ! -z "$INGRESS_HOSTNAME" ]; then
              echo "Ingress LoadBalancer found: $INGRESS_HOSTNAME"
              break
            fi
            echo "Waiting for Ingress LoadBalancer... (attempt $i/30)"
            sleep 20 # Wait a bit longer between checks
          done

          if [ -z "$INGRESS_HOSTNAME" ]; then
            echo "Error: Ingress LoadBalancer for argocd-server-ingress not found after timeout."
            kubectl get ingress argocd-server-ingress -n argocd -o yaml # For debugging
            kubectl describe ingress argocd-server-ingress -n argocd # For debugging
            exit 1
          fi

      - name: Delete existing ArgoCD DNS record (if exists)
        run: |
          ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          
          echo "Checking for existing A or CNAME record..."
          RECORD=$(aws route53 list-resource-record-sets \
            --hosted-zone-id $ZONE_ID \
            --query "ResourceRecordSets[?Name=='argocd.bunnycloud.xyz.'] | [?Type=='A' || Type=='CNAME']" \
            --output json)
          
          if [[ "$RECORD" != "[]" ]]; then
            echo "$RECORD" > existing_record.json
            cat existing_record.json | jq '{Changes: [{Action: "DELETE", ResourceRecordSet: .[0]}]}' > delete-change-batch.json
            
            echo "Deleting existing record..."
            aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID --change-batch file://delete-change-batch.json || echo "Delete failed (possibly due to mismatch)"
          else
            echo "No existing A or CNAME record found."
          fi

      - name: Update ArgoCD DNS record
        run: |
          # Get the LoadBalancer hostname
          LB_HOSTNAME=$(kubectl get svc argocd-server-lb -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          if [ -z "$LB_HOSTNAME" ]; then
            echo "Error: ArgoCD LoadBalancer not found"
            exit 1
          fi
          
          echo "ArgoCD LoadBalancer hostname: $LB_HOSTNAME"
          
          # Get the Route53 hosted zone ID
          ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          echo "Route53 zone ID: $ZONE_ID"
          
          # Get the LoadBalancer's hosted zone ID
          LB_ZONE_ID=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?DNSName=='$LB_HOSTNAME'].CanonicalHostedZoneId" --output text)
          
          if [ -z "$LB_ZONE_ID" ] || [ "$LB_ZONE_ID" == "None" ]; then
            echo "Trying classic ELB..."
            LB_ZONE_ID=$(aws elb describe-load-balancers --query "LoadBalancerDescriptions[?DNSName=='$LB_HOSTNAME'].CanonicalHostedZoneNameID" --output text)
          fi
          
          echo "LoadBalancer zone ID: $LB_ZONE_ID"
          
          # Update the Route53 record with an ALIAS record
          CHANGE_BATCH='{
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "argocd.bunnycloud.xyz",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "'$LB_ZONE_ID'",
                    "DNSName": "'$LB_HOSTNAME'",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }'
          echo "Change batch: $CHANGE_BATCH"
          
          CHANGE_RESULT=$(aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID --change-batch "$CHANGE_BATCH")
          echo "Change result: $CHANGE_RESULT"
          
          echo "DNS record updated for argocd.bunnycloud.xyz -> $LB_HOSTNAME"
          
          # Test the record using AWS nameservers
          echo "Testing DNS record using AWS nameservers..."
          aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID --query "ResourceRecordSets[?Name=='argocd.bunnycloud.xyz.']"
          dig @ns-94.awsdns-11.com argocd.bunnycloud.xyz

      - name: Check and update EKS node security group
        run: |
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Checking if security group allows inbound traffic on port 8080..."
          
          # Check if the rule exists
          RULE_EXISTS=$(aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`8080\` && ToPort==\`8080\` && IpProtocol==\`tcp\`]" --output text)
          
          # Note: This rule for port 8080 on nodes might be for a direct LoadBalancer service.
          # If using an Ingress controller (like Nginx or AWS LBC), the relevant ports would be
          # the NodePorts used by the Ingress controller service, and the ELB it creates should have access to them.
          
          if [ -z "$RULE_EXISTS" ]; then
            echo "Adding rule to allow inbound traffic on port 8080..."
            aws ec2 authorize-security-group-ingress \
              --group-id $SG_ID \
              --protocol tcp \
              --port 8080 \
              --cidr 0.0.0.0/0
            echo "Rule added successfully"
          else
            echo "Rule already exists"
          fi
          
      - name: Check domain nameserver delegation
        run: |
          echo "Checking nameserver delegation for bunnycloud.xyz..."
          
          # Get AWS Route53 nameservers for the zone
          ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          AWS_NS=$(aws route53 get-hosted-zone --id $ZONE_ID --query "DelegationSet.NameServers" --output text)
          echo "AWS Route53 nameservers for bunnycloud.xyz: $AWS_NS"
          
          # Get actual nameservers from public DNS
          ACTUAL_NS=$(dig +short NS bunnycloud.xyz)
          echo "Actual nameservers for bunnycloud.xyz: $ACTUAL_NS"
          
          # Check if they match
          for NS in $AWS_NS; do
            if ! echo "$ACTUAL_NS" | grep -q "$NS"; then
              echo "⚠️ WARNING: AWS nameserver $NS not found in actual nameservers"
            fi
          done
          
          echo "To fix this issue, update the nameservers at your domain registrar (Strato) to use the AWS Route53 nameservers listed above."
      - name: Test DNS record
        run: |
          echo "Testing DNS record for argocd.bunnycloud.xyz..."
          dig argocd.bunnycloud.xyz
          
          echo "Waiting for DNS propagation..."
          for i in {1..10}; do
            IP=$(dig +short argocd.bunnycloud.xyz)
            if [ ! -z "$IP" ]; then
              echo "DNS record found: argocd.bunnycloud.xyz -> $IP"
              break
            fi
            echo "Waiting for DNS propagation... (attempt $i/10)"
            sleep 30
          done
          
          # echo "Testing HTTPS access to argocd.bunnycloud.xyz..."
          # curl -vL --max-time 20 https://argocd.bunnycloud.xyz || echo "HTTPS access failed"

      - name: Check Route53 record
        run: |
          echo "Checking Route53 record for argocd.bunnycloud.xyz..."
          # ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID --query "ResourceRecordSets[?Name=='argocd.bunnycloud.xyz.']"

  generate-argo-apps:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python and install Jinja2
      run: |
        sudo apt-get update
        sudo apt-get install -y python3 python3-pip
        pip3 install jinja2-cli

    - name: Detect new app folders
      id: detect
      run: |
        echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT

    - name: Generate manifests
      run: |
        mkdir -p k8s/argocd/applications
        for app in $(echo '${{ steps.detect.outputs.apps }}' | jq -r '.[]'); do
          OUTFILE="k8s/argocd/applications/${app}.yaml"
          echo "Rendering $OUTFILE..."
          jinja2 manifests/templates/argocd-app.yaml.j2 \
            -D app_name="$app" \
            -D github_repository="${{ github.repository }}" \
            > "$OUTFILE"
        done

    - name: Commit & create PR
      uses: peter-evans/create-pull-request@v5
      with:
        commit-message: "Generate ArgoCD Application manifests"
        branch: auto/argo-apps
        title: "Generate ArgoCD Application manifests"
        body: |
          This PR contains ArgoCD Application manifests for newly detected applications.
          Each manifest was rendered using the provided Jinja2 template.
          Please review the generated files under `k8s/argocd/applications/` before merging.

  lint-and-deploy-helm:
    needs: docker
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Lint Helm charts
        run: |
          if [ -d "helm" ]; then
            for chart in helm/*; do
              if [ -d "$chart" ]; then
                echo "Linting $chart..."
                helm lint "$chart"
              fi
            done
          fi

      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "helm" ]; then
            for chart in helm/*; do
              if [ -d "$chart" ]; then
                echo "Rendering $chart..."
                helm template "$chart"
              fi
            done
          fi

      - name: Apply ArgoCD Application manifests
        run: |
          if [ -f "k8s/argocd/applications/nodejs.yaml" ]; then
            kubectl apply -f k8s/argocd/applications/nodejs.yaml
          else
            echo "Node.js ArgoCD application manifest not found in k8s/argocd/applications."
          fi
