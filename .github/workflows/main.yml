name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Do you want to destroy infrastructure?'
        required: true
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
  push:
    branches: [ main ]
    paths:
      - 'apps/**'
      - 'manifests/templates/**'
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/${{ secrets.DOCKER_USERNAME }}

jobs:

  build-and-test:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    permissions:
      actions: read # Default permission for GITHUB_TOKEN
      contents: read # To checkout the repository
      security-events: write # Required for CodeQL to upload SARIF results
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:javascript"

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: build-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/${{ env.APP_NAME }}:${{ github.sha }}

      # Scan the image after it has been built and loaded/pushed
      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}'
          format: 'table'
          exit-code: '1' # Fail the build if vulnerabilities are found (adjust severity as needed)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          # Ensure the scanner checks the local Docker daemon
          # Trivy action usually does this by default if image is found locally.

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}

      - name: Wait for app to be ready
        run: |
          for i in {1..20}; do
            curl --fail http://localhost:3000/status && exit 0
            echo "Waiting for app to be ready ($i/20)..."
            sleep 5
          done

  terraform-eks:
    # Only run terraform-eks when destroy is not true
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      # Only run the Terraform logic if destroy == 'true'
      - name: Terraform Destroy (only if destroy is true)
        if: ${{ github.event.inputs.destroy == 'true' }}
        run: |
          echo "Running Terraform destroy since destroy input is true"
          terraform init
          terraform destroy -auto-approve

      # Run the rest of the deployment logic only if destroy != 'true'
      - name: Checkout code
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Run tfsec IaC scan
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: aquasecurity/tfsec-action@v1.0.0 # Use a specific version if preferred
        with:
          working_directory: terraform
          addtional_args: '--format json --output-file tfsec-report.json'
          soft_fail: true # Uncomment to see warnings without failing the build initially
      
      - name: Upload tfsec report
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: tfsec-report
          path: terraform/tfsec-report.json
      - name: Terraform Init
        if: ${{ github.event.inputs.destroy != 'true' }}
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: terraform validate
      - name: Terraform Format
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: terraform fmt -check
      
      - name: Terraform Unlock
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: terraform force-unlock -force 1bcecd76-352b-7fd1-1dce-442e67ecb6c8 || echo "No locks to unlock"
      - name: Terraform Plan
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: terraform plan

      - name: Terraform Apply
        if: ${{ github.event.inputs.destroy != 'true' }}
        working-directory: terraform
        run: terraform apply -auto-approve

      - name: Generate kubeconfig and upload
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: |
          mkdir -p /tmp/kubeconfig
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example --kubeconfig /tmp/kubeconfig/config
        env:
          AWS_REGION: ap-southeast-2
        shell: bash

      - name: Upload kubeconfig as artifact
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: bastion-kubeconfig
          path: /tmp/kubeconfig/config

      - name: Upload deployment_key.pem as artifact
        if: ${{ github.event.inputs.destroy != 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem

      - name: Install kubectl and check EKS status
        if: ${{ github.event.inputs.destroy != 'true' }}
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes --request-timeout=60s
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system --request-timeout=60s

  
  destroy-infrastructure:
    if: ${{ github.event.inputs.destroy == 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install required tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example || true
      - name: Clean up AWS resources
        run: |
          echo "Cleaning up AWS resources..."
          
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query "Vpcs[0].VpcId" --output text || echo "")
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" == "None" ]; then
            echo "VPC not found, skipping cleanup"
            exit 0
          fi
          echo "VPC ID: $VPC_ID"
          
          # Delete load balancers within the specific VPC
          echo "Deleting AWS Load Balancers in VPC $VPC_ID..."
          aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | tr '\t' '\n' | while read LB_ARN; do
            if [ ! -z "$LB_ARN" ]; then
              echo "Deleting load balancer $LB_ARN..."
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            fi
          done
          
          # Wait for load balancers to be deleted
          echo "Waiting for AWS Load Balancers to be fully deleted..."
          sleep 120 # Increased wait time
          
          # Release Elastic IPs associated with the VPC (e.g., NAT Gateways if not managed by TF, or orphaned)
          # WARNING: This can be risky if not properly scoped. Terraform should manage its own EIPs.
          # This example attempts to find EIPs in the VPC that are not associated or associated with NATs.
          echo "Releasing unassociated or NAT Gateway Elastic IPs in VPC $VPC_ID (use with caution)..."
          aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text | tr '\t' '\n' | while read ALLOC_ID; do
            if [ ! -z "$ALLOC_ID" ]; then # Corrected variable from LB to ALLOC_ID
              # Check if EIP is associated with a NAT gateway or unassociated
              ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].AssociationId" --output text)
              # NETWORK_INTERFACE_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].NetworkInterfaceId" --output text) # Not used, can be removed
              IS_NAT_GW_EIP=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" "Name=nat-gateway-address.allocation-id,Values=$ALLOC_ID" --query "NatGateways" --output text)

              if [ "$ASSOCIATION_ID" == "None" ] || [ ! -z "$IS_NAT_GW_EIP" ] ; then
                echo "Attempting to release Elastic IP $ALLOC_ID..."
                # Disassociation might be needed if it's a NAT GW EIP that TF failed to delete
                if [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id "$ASSOCIATION_ID" || echo "Failed to disassociate $ALLOC_ID, might already be disassociated or managed elsewhere."
                    sleep 2
                fi
                aws ec2 release-address --allocation-id "$ALLOC_ID" || echo "Failed to release $ALLOC_ID, it might be in use or managed by Terraform."
              else
                echo "Skipping EIP $ALLOC_ID as it appears to be actively associated with a resource not identified as a NAT GW or unassociated."
              fi
            fi
          done
          
          # Get all subnets in the VPC
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text)
          echo "Subnets: $SUBNETS"
          echo "Note: Terraform destroy should handle the deletion of VPC-specific resources like NAT Gateways, EIPs, and Network Interfaces created by Terraform."
          
          # Wait for resources to be released
          echo "Waiting for resources to be released..."
          sleep 120 # Increased wait time

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Confirm before destroying infrastructure
        if: github.event.inputs.destroy == 'true'
        run: |
          echo "⚠️  You chose to destroy infrastructure."

      - name: Destroy Terraform-managed infrastructure
        if: github.event.inputs.destroy == 'true'
        run: |
          cd terraform
          terraform init
          terraform destroy -auto-approve

      - name: Clean up orphaned ELBs and their subnets (manual cleanup)
        if: github.event.inputs.destroy == 'true'
        run: |
          echo "🧹 Cleaning up leftover Load Balancers and subnets..."
          
          # Delete classic ELBs
          for elb in $(aws elb describe-load-balancers --query 'LoadBalancerDescriptions[].LoadBalancerName' --output text); do
            aws elb delete-load-balancer --load-balancer-name "$elb" || echo "Failed to delete ELB: $elb"
          done

          # Delete NLBs and ALBs
          for elbv2 in $(aws elbv2 describe-load-balancers --query 'LoadBalancers[].LoadBalancerArn' --output text); do
            aws elbv2 delete-load-balancer --load-balancer-arn "$elbv2" || echo "Failed to delete ELBv2: $elbv2"
          done

          # Delete unused subnets
          for subnet in $(aws ec2 describe-subnets --query 'Subnets[].SubnetId' --output text); do
            aws ec2 delete-subnet --subnet-id "$subnet" || echo "Skipping in-use subnet: $subnet"
          done

      
  deploy-argocd:
    if: ${{ github.event.inputs.destroy_infra != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Init Terraform
        working-directory: terraform
        run: terraform init

      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Add Helm repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Get ACM certificate ARN from Terraform
        run: |
          # Get the certificate ARN from Terraform output
          cd terraform
          CERT_ARN=$(terraform output -raw acm_certificate_arn || echo "")
          
          if [ -z "$CERT_ARN" ]; then
            echo "Warning: Could not get certificate ARN from Terraform output"
            # Fallback to looking up the certificate
            CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            
            if [ -z "$CERT_ARN" ]; then
              echo "Error: No certificate found for *.bunnycloud.xyz"
              exit 1
            fi
          fi
          
          echo "Using certificate ARN: $CERT_ARN"
          echo "ACM_CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV

      - name: Check NGINX Ingress Controller
        id: check-nginx
        run: |
          if kubectl get namespace ingress-nginx &>/dev/null; then
            if kubectl get deployment -n ingress-nginx ingress-nginx-controller &>/dev/null; then
              echo "NGINX Ingress Controller deployment exists"
              kubectl get pods -n ingress-nginx
              kubectl get svc -n ingress-nginx
              echo "nginx_exists=true" >> $GITHUB_OUTPUT
            else
              echo "nginx_exists=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "nginx_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Install NGINX Ingress Controller with SSL
        if: steps.check-nginx.outputs.nginx_exists != 'true'
        run: |
          echo "Installing NGINX Ingress Controller..."
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${{ env.ACM_CERTIFICATE_ARN }}" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
            --set controller.service.targetPorts.https=http \
            --set controller.service.ports.https=443 \
            --set controller.service.ports.http=80 \
            --set controller.service.enableHttp=true \
            --set controller.config.use-forwarded-headers=true \
            --set controller.config.compute-full-forwarded-for=true \
            --set controller.config.use-proxy-protocol=false \
            --set controller.config.enable-real-ip=false \
            --set controller.config.proxy-real-ip-cidr=0.0.0.0/0 \
            --set controller.config.server-tokens=false \
            --set controller.replicaCount=1 \
            --set controller.containerPort.http=80 \
            --set controller.containerPort.https=443 \
            --version 4.10.1 \
            --timeout 30m

          
            echo "Waiting for ingress-nginx controller pod to be ready..."
            kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=10m || true
            
            echo "Checking NGINX Ingress Controller pods..."
            kubectl get pods -n ingress-nginx
            
            echo "Checking NGINX Ingress Controller services..."
            kubectl get svc -n ingress-nginx
            
            echo "Checking NGINX Ingress Controller events..."
            kubectl get events -n ingress-nginx --sort-by='.lastTimestamp' | tail -20
            
            echo "Creating default backend for 404 handling..."
            cat > default-backend.yaml << 'ENDOFFILE'
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: default-http-backend
              namespace: ingress-nginx
              labels:
                app: default-http-backend
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: default-http-backend
              template:
                metadata:
                  labels:
                    app: default-http-backend
                spec:
                  containers:
                  - name: default-http-backend
                    image: k8s.gcr.io/defaultbackend-amd64:1.5
                    ports:
                    - containerPort: 8080
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: default-http-backend
              namespace: ingress-nginx
            spec:
              ports:
              - port: 80
                targetPort: 8080
              selector:
                app: default-http-backend
            ENDOFFILE
            kubectl apply -f default-backend.yaml
            
            echo "Configuring NGINX Ingress Controller to use default backend..."
            kubectl patch deployment ingress-nginx-controller -n ingress-nginx --type=json \
              -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--default-backend-service=ingress-nginx/default-http-backend"}]'
            
            echo "Restarting NGINX Ingress Controller..."
            kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx
            kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m
            
            echo "Applying ConfigMap patches to fix PROXY protocol issues..."
            kubectl patch configmap -n ingress-nginx ingress-nginx-controller --type=merge -p '{"data":{"use-proxy-protocol":"false"}}'
            kubectl patch configmap -n ingress-nginx ingress-nginx-controller --type=merge -p '{"data":{"proxy-real-ip-cidr":"0.0.0.0/0"}}'
            kubectl patch configmap -n ingress-nginx ingress-nginx-controller --type=merge -p '{"data":{"enable-real-ip":"false"}}'
            
            echo "Waiting for ConfigMap changes to propagate..."
            sleep 30
            
            echo "Restarting NGINX Ingress Controller again to apply ConfigMap changes..."
            kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx
            kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m
            
            echo "Creating a direct patch to NGINX configuration..."
            kubectl exec -n ingress-nginx deploy/ingress-nginx-controller -- cat /etc/nginx/nginx.conf > nginx.conf
            
            echo "Checking if PROXY protocol is enabled in NGINX configuration..."
            grep -i "proxy_protocol" nginx.conf || echo "No proxy_protocol found in configuration"
            
            echo "Verifying NGINX Ingress Controller configuration..."
            kubectl get configmap -n ingress-nginx ingress-nginx-controller -o yaml

      - name: Update ArgoCD DNS record
        run: |
          # Get the hosted zone ID from Terraform output
          cd terraform
          HOSTED_ZONE_ID=$(terraform output -raw route53_zone_id || echo "")
          
          if [ -z "$HOSTED_ZONE_ID" ]; then
            echo "Error: Could not get hosted zone ID from Terraform output"
            # Fallback to looking up the hosted zone
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
            
            if [ -z "$HOSTED_ZONE_ID" ]; then
              echo "Error: No hosted zone found for bunnycloud.xyz"
              exit 1
            fi
          fi
          
          echo "Using hosted zone ID: $HOSTED_ZONE_ID"
          
          # Get the NGINX LoadBalancer hostname
          NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          if [ -z "$NGINX_LB_HOSTNAME" ]; then
            echo "Error: Could not get NGINX Ingress Controller LoadBalancer hostname"
            exit 1
          fi
          echo "Using NGINX LoadBalancer hostname: $NGINX_LB_HOSTNAME"
          
          # Update ArgoCD DNS record
          echo "DNS record for argocd.bunnycloud.xyz will be managed by ExternalDNS via Ingress annotations in the ArgoCD Helm chart."

          kubectl get pods -n ingress-nginx

      - name: Patch ArgoCD resources with Helm ownership
        run: bash k8s/argocd/patch-argocd-helm-ownership.sh

      - name: Get ArgoCD IAM Role ARN
        run: |
          cd terraform
          ARGOCD_ROLE_ARN=$(terraform output -raw argocd_cognito_role_arn || echo "")
          if [ -z "$ARGOCD_ROLE_ARN" ]; then
            echo "Error: Could not get ArgoCD role ARN from Terraform output"
            exit 1
          fi
          echo "Using ArgoCD role ARN: $ARGOCD_ROLE_ARN"
          echo "ARGOCD_ROLE_ARN=$ARGOCD_ROLE_ARN" >> $GITHUB_ENV
          
          # Get Cognito details
          echo "Fetching Cognito details..."
          COGNITO_CLIENT_ID=$(terraform output -raw cognito_user_pool_client_id || echo "")
          COGNITO_ISSUER_URL=$(terraform output -raw cognito_user_pool_endpoint || echo "")
          
          echo "Fetching Cognito client secret from Secrets Manager..."
          COGNITO_CLIENT_SECRET=$(aws secretsmanager get-secret-value \
            --secret-id argocd-cognito-client-secret \
            --region ap-southeast-2 \
            --query SecretString \
            --output text 2>/dev/null)

          if [ -z "$COGNITO_CLIENT_SECRET" ]; then
            echo "❌ Error: Cognito client secret not found!"
            exit 1
          fi

          echo "✅ Cognito details retrieved."
          echo "COGNITO_CLIENT_ID=$COGNITO_CLIENT_ID" >> $GITHUB_ENV
          echo "COGNITO_ISSUER_URL=$COGNITO_ISSUER_URL" >> $GITHUB_ENV
          echo "COGNITO_CLIENT_SECRET=$COGNITO_CLIENT_SECRET" >> $GITHUB_ENV

     
      - name: Install/Upgrade ArgoCD with IAM role and Ingress
        run: |
          # Debug the ARN value
          echo "ARN value: ${{ env.ARGOCD_ROLE_ARN }}"
          
          # Create a values file for the installation
          cat > argocd-install-values.yaml <<EOF
          global:
            dualStack:
              enabled: false
          # Disable Dex if using Cognito as the primary OIDC provider
          dex:
            enabled: false
          # Example: Disable notifications controller if not used, to save resources
          # notifications:
          #   enabled: false
          # Example: Annotate ApplicationSet controller's SA if it needs AWS permissions
          # applicationSet:
          #   serviceAccount:
          #     annotations:
          #       eks.amazonaws.com/role-arn: "${{ env.ARGOCD_ROLE_ARN }}" # Or a more specific role
          server:
            serviceAccount:
              create: true
              name: argocd-server
              annotations:
                eks.amazonaws.com/role-arn: "${{ env.ARGOCD_ROLE_ARN }}"
            ingress:
              enabled: true
              ingressClassName: nginx
              hosts:
                - argocd.bunnycloud.xyz
              annotations:
                kubernetes.io/ingress.class: nginx
                nginx.ingress.kubernetes.io/ssl-redirect: "true"
                external-dns.alpha.kubernetes.io/hostname: "argocd.bunnycloud.xyz"
              tls:
                - hosts:
                    - argocd.bunnycloud.xyz
                  secretName: argocd-tls
          # Annotate other SAs if they need AWS permissions (e.g., for specific Git repo access or cluster management tasks)
          controller:
            serviceAccount:
              annotations:
                eks.amazonaws.com/role-arn: "${{ env.ARGOCD_ROLE_ARN }}" # Use the main ArgoCD role or a more specific one
          repoServer:
            serviceAccount:
              annotations:
                eks.amazonaws.com/role-arn: "${{ env.ARGOCD_ROLE_ARN }}" # Use the main ArgoCD role or a more specific one
          # Ensure other necessary ArgoCD configurations are here
          EOF
          
          # Install or upgrade ArgoCD with the values file
          # Use upgrade --install for idempotency
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --values argocd-install-values.yaml \
            --version 6.11.1 \
            --timeout 10m \
            --wait
          
          # Verify the annotation was set correctly
          kubectl get serviceaccount argocd-server -n argocd -o yaml

      - name: Validate ArgoCD IAM role integration
        run: |
          echo "Validating ArgoCD IAM role integration..."
          
          # Check if service account exists
          if ! kubectl get sa argocd-server -n argocd &>/dev/null; then
            echo "Error: Service account argocd-server not found in namespace argocd"
            exit 1
          fi

          # Check if service account has the correct annotation
          SA_ANNOTATION=$(kubectl get sa argocd-server -n argocd -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}')
          if [ "$SA_ANNOTATION" != "${{ env.ARGOCD_ROLE_ARN }}" ]; then
            echo "Error: Service account annotation does not match expected role ARN"
            echo "Expected: ${{ env.ARGOCD_ROLE_ARN }}"
            echo "Actual: $SA_ANNOTATION"
            exit 1
          fi

          echo "✅ ArgoCD IAM role integration validated"
    
      - name: Configure ArgoCD with Cognito
        run: |
          # Set up ArgoCD admin password (can also be managed via Helm values if chart supports it well)
          # This ensures an admin user is available.
          echo "Setting up ArgoCD admin password..."
          ADMIN_PASSWORD="admin" # For production, use a more secure method or a secret
          kubectl -n argocd create secret generic argocd-initial-admin-secret \
            --from-literal=password=$ADMIN_PASSWORD \
            --from-literal=clearPassword=$ADMIN_PASSWORD \
            --dry-run=client -o yaml | kubectl apply -f - || echo "Admin secret 'argocd-initial-admin-secret' might already exist or failed to apply."
          kubectl -n argocd patch configmap argocd-cm --type=merge -p '{"data":{"admin.enabled":"true"}}'

          # Fix the server URL first
          kubectl -n argocd patch configmap argocd-cm --type=merge -p '{"data":{"url":"https://argocd.bunnycloud.xyz"}}'
          
          # Generate a server secret key if missing
          kubectl -n argocd patch secret argocd-secret --type=merge -p '{"stringData":{"server.secretkey":"'$(openssl rand -base64 32)'"}}'
          
          # Configure Cognito OIDC
          kubectl -n argocd patch configmap argocd-cm --type=merge -p '{
            "data": {
              "oidc.config": "name: Cognito\nissuer: ${{ env.COGNITO_ISSUER_URL }}\nclientID: ${{ env.COGNITO_CLIENT_ID }}\nclientSecret: $oidc.cognito.clientSecret\nrequestedScopes: [\"openid\", \"profile\", \"email\"]\nrequestedIDTokenClaims: {\"groups\": {\"essential\": true}}"
            }
          }'
          
          # Store client secret in argocd-secret
          kubectl -n argocd patch secret argocd-secret --type=merge -p '{
            "stringData": {
              "oidc.cognito.clientSecret": "${{ env.COGNITO_CLIENT_SECRET }}"
            }
          }'
          
          # Configure RBAC for Cognito users
          kubectl -n argocd patch configmap argocd-rbac-cm --type=merge -p '{
            "data": {
              "policy.csv": "g, Admins, role:admin\ng, Developers, role:readonly",
              "scopes": "[cognito:groups]"
            }
          }'
          
          # Get list of available deployments
          echo "Available ArgoCD deployments:"
          kubectl get deployments -n argocd
          
          # Restart only existing deployments
          for deployment in $(kubectl get deployments -n argocd -o jsonpath='{.items[*].metadata.name}'); do
            echo "Restarting deployment $deployment"
            kubectl -n argocd rollout restart deployment/$deployment
          done
          
          # Wait for ArgoCD server to be ready (continue on error)
          echo "Waiting for ArgoCD server to restart..."
          kubectl -n argocd wait --for=condition=Available deployment/argocd-server --timeout=2m || true
          
          echo "ArgoCD configured with Cognito OIDC."
          echo "Users can now log in using their Cognito credentials"
          echo "Admin user (if enabled): admin / Password: $ADMIN_PASSWORD (or as set)"

      - name: Get ExternalDNS role ARN from Terraform
        run: |
          echo "Getting ExternalDNS role ARN from Terraform..."
          cd terraform

          # Initialize Terraform to access the state
          # The backend configuration in your Terraform files must allow this job to access the state
          echo "Initializing Terraform..."
          if ! terraform init -input=false -reconfigure; then
            echo "Error: Terraform init failed."
            # Decide if you want to exit or attempt fallback. Exiting is safer.
            # exit 1 
          fi

          ROLE_ARN_FROM_TF=$(terraform output -raw external_dns_role_arn 2>/dev/null)

          if [ ! -z "$ROLE_ARN_FROM_TF" ]; then
            echo "Successfully retrieved ExternalDNS role ARN from Terraform output: $ROLE_ARN_FROM_TF"
            ROLE_ARN="$ROLE_ARN_FROM_TF"
          else
            echo "Warning: Could not get ExternalDNS role ARN from Terraform output. Attempting fallback."
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
            if [ -z "$AWS_ACCOUNT_ID" ]; then
              echo "Error: Could not determine AWS Account ID for fallback."
              exit 1
            fi
            # This ROLE_NAME must exactly match the name defined in terraform/external_dns.tf
            FALLBACK_ROLE_NAME="eks-external-dns-role" 
            ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${FALLBACK_ROLE_NAME}"
            echo "Using fallback ExternalDNS role ARN: $ROLE_ARN"
            # Verify the fallback role actually exists
            if ! aws iam get-role --role-name "$FALLBACK_ROLE_NAME" > /dev/null 2>&1; then
                echo "Error: Fallback role '$FALLBACK_ROLE_NAME' does not exist in AWS."
                echo "Please check Terraform apply job and ensure the role was created with this name."
                exit 1
            fi
          fi
          
          echo "Using ExternalDNS role ARN: $ROLE_ARN"
          echo "EXTERNAL_DNS_ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV
          
          # Get hosted zone ID - applying similar robustness
          echo "Getting Hosted Zone ID..."
          HOSTED_ZONE_ID_FROM_TF=$(terraform output -raw route53_zone_id 2>/dev/null)

          if [ ! -z "$HOSTED_ZONE_ID_FROM_TF" ]; then
            echo "Successfully retrieved Hosted Zone ID from Terraform output: $HOSTED_ZONE_ID_FROM_TF"
            HOSTED_ZONE_ID="$HOSTED_ZONE_ID_FROM_TF"
          else
            echo "Warning: Could not get Hosted Zone ID from Terraform output. Attempting fallback."
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          fi

          if [ -z "$ROLE_ARN" ]; then
            echo "Error: ExternalDNS role ARN is empty after all attempts."
            exit 1
          fi
          echo "Using hosted zone ID: $HOSTED_ZONE_ID"
          echo "HOSTED_ZONE_ID=$HOSTED_ZONE_ID" >> $GITHUB_ENV
          
      - name: Cleanup DNS conflicts before ExternalDNS install
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          HOSTED_ZONE_ID="${{ env.HOSTED_ZONE_ID }}"
          
          # Define all base hostnames that ExternalDNS will manage
          BASE_HOSTNAMES=(
            "github-actions-example.bunnycloud.xyz"
            "argocd.bunnycloud.xyz"
          )

          RECORDS_TO_QUERY_AND_DELETE=()

          for HOSTNAME_BASE in "${BASE_HOSTNAMES[@]}"; do
            HOSTNAME_FQDN="${HOSTNAME_BASE}."
            RECORDS_TO_QUERY_AND_DELETE+=(
              "${HOSTNAME_FQDN}" # A or CNAME for the hostname itself
              "externaldns-cname-${HOSTNAME_FQDN}" # ExternalDNS TXT record for CNAME ownership
              "externaldns-${HOSTNAME_FQDN}"       # ExternalDNS TXT record for A record ownership (if applicable)
            )
          done
          
          echo "Will query and attempt to delete the following records if they exist:"
          printf "%s\n" "${RECORDS_TO_QUERY_AND_DELETE[@]}"

          CHANGES_JSON="[]"

          for RECORD_NAME_TO_CLEAN in "${RECORDS_TO_QUERY_AND_DELETE[@]}"; do
            echo "🔍 Checking for records in Route 53 for $RECORD_NAME_TO_CLEAN..."
            # Ensure we only attempt to delete records that actually exist and have ResourceRecords
            EXISTING_RECORDS_FOR_NAME=$(aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
              --query "ResourceRecordSets[?Name == '$RECORD_NAME_TO_CLEAN']" --output json)

            if [ "$EXISTING_RECORDS_FOR_NAME" != "[]" ]; then
              echo "⚠️ Records found for $RECORD_NAME_TO_CLEAN. Preparing deletion changes..."
              # Filter for A, CNAME, TXT types and ensure ResourceRecords exist
              # This jq query processes each record found for the given name
              DELETION_CHANGES_FOR_NAME=$(echo "$EXISTING_RECORDS_FOR_NAME" | jq '[.[] | select((.Type == "A" or .Type == "CNAME" or .Type == "TXT") and .ResourceRecords and (.ResourceRecords | length > 0)) | {
                Action: "DELETE",
                ResourceRecordSet: {
                  Name: .Name,
                  Type: .Type,
                  TTL: .TTL, # Use the actual TTL of the record to be deleted
                  ResourceRecords: .ResourceRecords
                }
              }]')
              
              if [ "$(echo "$DELETION_CHANGES_FOR_NAME" | jq 'length')" -gt 0 ]; then
                  CHANGES_JSON=$(echo "$CHANGES_JSON $DELETION_CHANGES_FOR_NAME" | jq -s 'add')
              fi
            else
              echo "✅ No records found for $RECORD_NAME_TO_CLEAN."
            fi
          done

          if [ "$(echo "$CHANGES_JSON" | jq 'length')" -eq 0 ]; then
            echo "✅ No DNS records found that require cleanup."
            exit 0
          fi

          echo "📦 Combined deletion changes to be applied:"
          echo "$CHANGES_JSON" | jq .

          cat > delete-conflicts.json <<EOF
          {
            "Comment": "Deleting potentially conflicting DNS records for managed hostnames and their ExternalDNS TXT records",
            "Changes": $CHANGES_JSON
          }
          EOF

          echo "🚨 Deleting records..."
          aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
            --change-batch file://delete-conflicts.json

          echo "✅ Records deletion request submitted."
      - name: Install ExternalDNS
        run: |
          echo "Installing ExternalDNS..."
          # Install ExternalDNS
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace external-dns --create-namespace \
            --set sources[0]=service \
            --set sources[1]=ingress \
            --set aws.zoneType=public \
            --set aws.region=ap-southeast-2 \
            --set domainFilters[0]=bunnycloud.xyz \
            --set policy=upsert-only \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.EXTERNAL_DNS_ROLE_ARN }}" \
            --set txtOwnerId="${{ env.HOSTED_ZONE_ID }}" \
            --set aws.zoneIdFilters[0]="${{ env.HOSTED_ZONE_ID }}" \
            --set provider=aws \
            --set aws.preferCNAME=true \
            --set txtPrefix=externaldns- \
            --set aws.batchChangeSize=1000 \
            --set logLevel=debug \
            --version 1.14.5 \
            --timeout 20m \
            --wait
          
          echo "ExternalDNS installed."

      - name: Retry ExternalDNS install if deployment fails
        run: |
          echo "Checking if ExternalDNS deployment exists..."
          if ! kubectl rollout status deployment external-dns -n external-dns --timeout=30s; then
            echo "ExternalDNS deployment failed. Reinstalling..."
            helm uninstall external-dns -n external-dns || true
            sleep 10
            kubectl delete secret -n external-dns $(kubectl get secrets -n external-dns | grep sh.helm.release.v1.external-dns | awk '{print $1}') || true
            sleep 5
            helm upgrade --install external-dns external-dns/external-dns \
              --namespace external-dns --create-namespace \
              --set sources[0]=service \
              --set sources[1]=ingress \
              --set aws.zoneType=public \
              --set aws.region=ap-southeast-2 \
              --set domainFilters[0]=bunnycloud.xyz \
              --set policy=upsert-only \
              --set serviceAccount.create=true \
              --set serviceAccount.name=external-dns \
              --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.EXTERNAL_DNS_ROLE_ARN }}" \
              --set txtOwnerId="${{ env.HOSTED_ZONE_ID }}" \
              --set provider=aws \
              --set txtPrefix=externaldns- \
              --set aws.batchChangeSize=1000 \
              --set logLevel=debug \
              --version 1.14.5 \
              --timeout 10m \
              --wait
          else
            echo "ExternalDNS is already deployed and healthy."
          fi
          
      - name: Verify ExternalDNS Service Account and Restart Pods
        run: |
          echo "Verifying ExternalDNS service account annotations..."
          kubectl describe sa external-dns -n external-dns
          echo "Restarting ExternalDNS deployment to ensure IRSA is applied..."
          kubectl rollout restart deployment external-dns -n external-dns
          kubectl rollout status deployment external-dns -n external-dns --timeout=2m

          # Check ExternalDNS logs
          echo "Checking ExternalDNS logs..."
          sleep 10  # Wait for ExternalDNS to start
          kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=100
          
          # Check ExternalDNS configuration
          echo "Checking ExternalDNS configuration..."
          kubectl get deployment external-dns -n external-dns -o yaml | grep -A20 args
          
      - name: Verify DNS records
        run: |
          HOSTED_ZONE_ID="${{ env.HOSTED_ZONE_ID }}"
          ARGO_RECORD_NAME="argocd.bunnycloud.xyz."
          APP_RECORD_NAME="github-actions-example.bunnycloud.xyz."
          
          echo "Verifying DNS records for $ARGO_RECORD_NAME (managed by ExternalDNS/manual step)..."
          aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
            --query "ResourceRecordSets[?Name=='$ARGO_RECORD_NAME']" --output json
            
          echo "Verifying DNS records for $APP_RECORD_NAME (should be managed by ExternalDNS via Ingress)..."
          aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
            --query "ResourceRecordSets[?Name=='$APP_RECORD_NAME']" --output json

      - name: Verify ArgoCD Installation and DNS
        run: |
          echo "Checking ArgoCD installation status..."
          kubectl get pods -n argocd

          echo "Checking ArgoCD service..."
          kubectl get svc -n argocd

          echo "Checking ArgoCD ingress..."
          kubectl get ingress -n argocd

          echo "Waiting for NGINX Ingress Controller LoadBalancer to get an address..."
          NGINX_LB_HOSTNAME=""
          RETRIES=15

          for i in $(seq 1 $RETRIES); do
            NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath="{.status.loadBalancer.ingress[0].hostname}" 2>/dev/null)

            if [ -n "$NGINX_LB_HOSTNAME" ]; then
              echo "✅ NGINX LoadBalancer hostname found: $NGINX_LB_HOSTNAME"
              break
            fi

            echo "⏳ Attempt $i/$RETRIES: Waiting for LoadBalancer hostname..."
            sleep 20
          done

          if [ -z "$NGINX_LB_HOSTNAME" ]; then
            echo "❌ Error: Could not find NGINX Ingress Controller LoadBalancer hostname after timeout."
            exit 1
          fi

          echo "🔍 Verifying DNS record for argocd.bunnycloud.xyz (should point to $NGINX_LB_HOSTNAME)..."
          echo "Waiting for DNS propagation (max 5 minutes)..."
          for i in {1..10}; do
            EXPECTED_CNAME_TARGET="${NGINX_LB_HOSTNAME}."
            RESOLVED_CNAME_VALUE=$(dig +short -t CNAME argocd.bunnycloud.xyz @8.8.8.8)

            if [ "$RESOLVED_CNAME_VALUE" = "$EXPECTED_CNAME_TARGET" ]; then
              echo "✅ DNS CNAME record for argocd.bunnycloud.xyz propagated and correctly points to $NGINX_LB_HOSTNAME."
              break
            fi

            echo "⏳ Attempt $i/10: DNS not ready. Found: '$RESOLVED_CNAME_VALUE', Expected: '$EXPECTED_CNAME_TARGET'"
            sleep 30

            if [ $i -eq 10 ]; then
              echo "⚠️ Warning: DNS propagation for argocd.bunnycloud.xyz timed out or record mismatch."
              echo "Current DNS resolution:"
              dig -t CNAME argocd.bunnycloud.xyz @8.8.8.8
              dig argocd.bunnycloud.xyz +trace
            fi
          done
          
      # Security group rules are now managed by Terraform in eks_sg_rules.tf
      - name: Verify EKS security group rules
        run: |
          echo "Verifying EKS security group rules (now managed by Terraform)..."
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Cluster security group: $SG_ID"
          
          # Just verify the rules exist
          echo "Checking if security group allows required inbound traffic..."
          aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`443\` || FromPort==\`8080\`]" --output table
        continue-on-error: true  # Don't fail the workflow if verification fails

  generate-argo-apps:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python and install Jinja2
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install jinja2-cli
      # Ensure yq is installed if you need it for other YAML processing
      # sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq
      - name: Detect new app folders
        id: detect
        run: |
          echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
          
      - name: Generate manifests
        run: |
          mkdir -p k8s/argo          APPS_JSON='${{ steps.detect.outputs.apps }}'
          
          # Use repository variables or defaults
          DOCKER_USERNAME="${{ secrets.DOCKER_USERNAME || '1073286' }}"
          DEPLOYMENT_NAMESPACE="${{ vars.DEPLOYMENT_NAMESPACE || 'github-actions-example' }}"
          IMAGE_NAME="github-actions-example"
          ACM_CERT_ARN="${{ env.ACM_CERTIFICATE_ARN }}"

          echo "$APPS_JSON" | jq -r '.[]' | while read app; do
            OUTFILE="k8s/argocd/applications/${app}.yaml"
            echo "Rendering $OUTFILE for app: $app"

            # Define variables
            IMAGE_REPOSITORY="${DOCKER_USERNAME}/${IMAGE_NAME}"
            IMAGE_TAG="${{ github.sha }}"
            APP_NAME="github-actions-example"

            jinja2 manifests/templates/argocd-app.yaml.j2 \
              -D app_name="$APP_NAME" \
              -D chart_path="apps/$app/chart" \
              -D github_repository_url="${{ github.server_url }}/${{ github.repository }}" \
              -D target_revision="${{ github.ref_name }}" \
              -D image_repository="$IMAGE_REPOSITORY" \
              -D image_tag="$IMAGE_TAG" \
              -D deployment_namespace="$DEPLOYMENT_NAMESPACE" \
              -D acm_certificate_arn="$ACM_CERT_ARN" \
              -D elb_hostname="${{ env.NGINX_LB_HOSTNAME }}" \
              > "$OUTFILE"
            echo "Generated manifest for $app (using app_name=$APP_NAME):"
            cat "$OUTFILE"
          done


      - name: Commit generated ArgoCD Application manifests
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add k8s/argocd/applications/
          if ! git diff --staged --quiet; then
            git commit -m "Generate/Update ArgoCD Application manifests for ${{ github.sha }}"
            git push origin HEAD:${{ github.ref_name }}
            echo "Committed and pushed ArgoCD Application manifests to ${{ github.ref_name }}."
          else
            echo "No changes to ArgoCD Application manifests to commit."
          fi

  lint-and-deploy-helm:
    if: ${{ github.event.inputs.destroy_infra != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: [docker, deploy-argocd] # Changed from generate-argo-apps to deploy-argocd
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Lint Helm charts
        run: |
          if [ -d "apps" ]; then
            for chart_dir in apps/*/chart; do
              echo "Linting $chart_dir..."
              helm lint $chart_dir
            done
          else
            echo "No apps directory found, skipping Helm lint"
          fi

      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "apps" ]; then
            for chart_dir in apps/*; do
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then
                echo "Rendering $chart_dir/chart..."
                helm template "$chart_dir/chart"
              fi
            done
          fi
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

      - name: Apply ArgoCD Application manifests
        run: |
          if [ -d "k8s/argocd/applications" ]; then
            for app_manifest in k8s/argocd/applications/*.yaml; do
              if [ -f "$app_manifest" ]; then
                kubectl apply -f "$app_manifest"
              fi
            done
          else
            echo "No ArgoCD application manifests found in k8s/argocd/applications."
          fi
      
      - name: Check Application Ingress Resource
        run: |
          APP_NAMESPACE="github-actions-example"
          APP_INGRESS_NAME="github-actions-example" # Assuming Ingress name matches app name

          echo "Waiting for namespace '$APP_NAMESPACE' to be created by ArgoCD..."
          for i in {1..30}; do # Wait up to 5 minutes (30 * 10s)
            if kubectl get namespace "$APP_NAMESPACE" > /dev/null 2>&1; then
              echo "Namespace '$APP_NAMESPACE' found."
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Error: Namespace '$APP_NAMESPACE' not found after waiting."
              exit 1
            fi
            echo "Namespace '$APP_NAMESPACE' not yet found, waiting... (Attempt $i/30)"
            sleep 10
          done

          echo "Waiting for Ingress '$APP_INGRESS_NAME' in namespace '$APP_NAMESPACE' to be created by ArgoCD..."
          for i in {1..60}; do # Wait up to 10 minutes (60 * 10s)
            if kubectl get ingress -n "$APP_NAMESPACE" "$APP_INGRESS_NAME" -o yaml > /dev/null 2>&1; then
              echo "Ingress '$APP_INGRESS_NAME' found."
              INGRESS_YAML=$(kubectl get ingress -n "$APP_NAMESPACE" "$APP_INGRESS_NAME" -o yaml)
              echo "Ingress YAML:"
              echo "$INGRESS_YAML"
              
              echo "Verifying ExternalDNS hostname annotation on Ingress..."
              ANNOTATION_VALUE=$(kubectl get ingress -n "$APP_NAMESPACE" "$APP_INGRESS_NAME" -o jsonpath='{.metadata.annotations.external-dns\.alpha\.kubernetes\.io/hostname}' 2>/dev/null)
              if [ -n "$ANNOTATION_VALUE" ]; then
                echo "ExternalDNS hostname annotation found: $ANNOTATION_VALUE"
                exit 0 # Success, Ingress and annotation found
              else
                echo "Warning: ExternalDNS hostname annotation not found on Ingress '$APP_INGRESS_NAME'. Will retry... (Attempt $i/60)"
              fi
            fi
            if [ $i -eq 60 ]; then
              echo "Error: Ingress '$APP_INGRESS_NAME' or its ExternalDNS annotation not found after waiting."
              kubectl get ingress -n "$APP_NAMESPACE" "$APP_INGRESS_NAME" -o yaml || echo "Ingress '$APP_INGRESS_NAME' still not found in namespace '$APP_NAMESPACE'."
              exit 1
            fi
            echo "Ingress '$APP_INGRESS_NAME' not yet found or annotation missing in namespace '$APP_NAMESPACE', waiting... (Attempt $i/60)"
            sleep 10
          done
      - name: Ensure application is deployed
        run: |
          APP_NAMESPACE="github-actions-example"
          APP_NAME="github-actions-example"
          
          kubectl create namespace $APP_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
          kubectl get pods -n $APP_NAMESPACE
          kubectl rollout status deployment/$APP_NAME -n $APP_NAMESPACE --timeout=2m || true

          echo "Service for $APP_NAME should be managed by ArgoCD via the Helm chart."

      - name: List ExternalDNS TXT records
        run: |
          # Get hosted zone ID
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          APP_DOMAIN="github-actions-example.bunnycloud.xyz."
          
          echo "Listing ExternalDNS TXT records..."
          
          # List ExternalDNS TXT records
          for txt_record in "externaldns-cname-github-actions-example.bunnycloud.xyz." "externaldns-github-actions-example.bunnycloud.xyz."; do
            echo "Checking for TXT record: $txt_record"
            aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
              --query "ResourceRecordSets[?Name=='$txt_record' && Type=='TXT']"
          done
          
      - name: Verify DNS record for application
        run: |
          APP_TARGET_HOSTNAME="github-actions-example.bunnycloud.xyz"
          NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
          
          if [ -z "$NGINX_LB_HOSTNAME" ]; then
            echo "Error: Could not find NGINX Ingress Controller LoadBalancer hostname."
            kubectl get svc -n ingress-nginx
            exit 1
          fi
          
          echo "NGINX LoadBalancer hostname: $NGINX_LB_HOSTNAME"
          echo "Checking DNS record for $APP_TARGET_HOSTNAME..."
          
          echo "Waiting for DNS CNAME propagation (ExternalDNS should create this)..."
          EXPECTED_CNAME_TARGET_FOR_DIG_SHORT="${NGINX_LB_HOSTNAME}." 

          for i in {1..12}; do
            RESOLVED_CNAME_VALUE=$(dig +short -t CNAME $APP_TARGET_HOSTNAME @8.8.8.8)
            if [ "$RESOLVED_CNAME_VALUE" = "$EXPECTED_CNAME_TARGET_FOR_DIG_SHORT" ]; then
              echo "✅ DNS CNAME record for $APP_TARGET_HOSTNAME propagated and correctly points to $NGINX_LB_HOSTNAME."
              break
            fi
            echo "Waiting for DNS CNAME propagation for $APP_TARGET_HOSTNAME... Attempt $i/12. Found: '$RESOLVED_CNAME_VALUE', Expected: '$EXPECTED_CNAME_TARGET_FOR_DIG_SHORT'"
            if [ $i -eq 12 ]; then
              echo "Warning: DNS CNAME propagation for $APP_TARGET_HOSTNAME timed out or record does not point to $NGINX_LB_HOSTNAME."
              echo "Final DNS CNAME resolution check for $APP_TARGET_HOSTNAME (using 8.8.8.8):"
              dig -t CNAME $APP_TARGET_HOSTNAME @8.8.8.8
              echo "Final DNS A record resolution check for $APP_TARGET_HOSTNAME (using 8.8.8.8):"
              dig -t A $APP_TARGET_HOSTNAME @8.8.8.8
              echo "Hint: Check ExternalDNS logs: kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=200"
              echo "Hint: Check Ingress annotations: kubectl get ingress github-actions-example -n github-actions-example -o yaml"
            fi
            sleep 10
          done
          
          # Wait for ingress to be properly configured
          echo "Brief wait for Ingress object to be processed by controller if recently updated by ArgoCD..."
          sleep 15

      - name: Verify application accessibility
        run: |
          APP_HOSTNAME="github-actions-example.bunnycloud.xyz"
          NGINX_LB_HOSTNAME=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
          
          echo "Testing DNS resolution for $APP_HOSTNAME..."
          nslookup $APP_HOSTNAME || echo "DNS resolution failed"
          
          echo "Testing direct connection to load balancer..."
          curl -v --connect-timeout 10 -k "http://$NGINX_LB_HOSTNAME" || echo "Connection to load balancer failed"
          
          # Wait longer for DNS and ingress to be fully configured
          echo "Waiting for DNS and ingress to be fully configured..."
          for i in {1..12}; do
            echo "Attempt $i/12: Testing connection to https://$APP_HOSTNAME..."
            if curl -s --connect-timeout 10 -k "https://$APP_HOSTNAME" | grep -q "github-actions-example"; then
              echo "✅ Application is accessible!"
              break
            fi
            echo "Application not yet accessible, waiting..."
            sleep 10
          done
        continue-on-error: true

      - name: Verify ingress configuration
        run: |
          echo "Verifying ingress configuration..."
          INGRESS_CONFIG=$(kubectl get ingress github-actions-example -n github-actions-example -o yaml)
          
          # Check for TLS section
          if echo "$INGRESS_CONFIG" | grep -q "tls:"; then
            echo "Error: TLS section found in ingress"
            exit 1
          fi
          
          # Check SSL redirect settings - more robust approach
          SSL_REDIRECT=$(kubectl get ingress github-actions-example -n github-actions-example -o jsonpath='{.metadata.annotations.nginx\.ingress\.kubernetes\.io/ssl-redirect}')
          FORCE_SSL_REDIRECT=$(kubectl get ingress github-actions-example -n github-actions-example -o jsonpath='{.metadata.annotations.nginx\.ingress\.kubernetes\.io/force-ssl-redirect}')
          
          echo "SSL redirect: $SSL_REDIRECT"
          echo "Force SSL redirect: $FORCE_SSL_REDIRECT"
          
          if [ "$SSL_REDIRECT" != "false" ] || [ "$FORCE_SSL_REDIRECT" != "false" ]; then
            echo "Error: SSL redirect not properly disabled"
            exit 1
          fi
          
          echo "✅ Ingress configuration verified"
