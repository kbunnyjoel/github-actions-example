name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Set to true to destroy the EKS cluster'
        required: false
        default: 'false'
      run_terraform:
        description: 'Run Terraform? (true/false)'
        required: false
        default: 'true'
  push:
    branches: [ main ]
    paths:
      - 'apps/**'
      - 'manifests/templates/**'
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/${{ secrets.DOCKER_USERNAME }}

jobs:

  build-and-test:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    permissions:
      actions: read # Default permission for GITHUB_TOKEN
      contents: read # To checkout the repository
      security-events: write # Required for CodeQL to upload SARIF results
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:javascript"

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: build-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/${{ env.APP_NAME }}:${{ github.sha }}

      # Scan the image after it has been built and loaded/pushed
      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}'
          format: 'table'
          exit-code: '1' # Fail the build if vulnerabilities are found (adjust severity as needed)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          # Ensure the scanner checks the local Docker daemon
          # Trivy action usually does this by default if image is found locally.

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}

      - name: Wait for app to be ready
        run: |
          for i in {1..20}; do
            curl --fail http://localhost:3000/status && exit 0
            echo "Waiting for app to be ready ($i/20)..."
            sleep 5
          done

  terraform-eks:
    if: ${{ github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Run tfsec IaC scan
        uses: aquasecurity/tfsec-action@v1.0.0 # Use a specific version if preferred
        with:
          working_directory: terraform
          addtional_args: '--format json --output-file tfsec-report.json'
          soft_fail: true # Uncomment to see warnings without failing the build initially
      
      - name: Upload tfsec report
        uses: actions/upload-artifact@v4
        with:
          name: tfsec-report
          path: terraform/tfsec-report.json
      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        run: terraform validate
      - name: Terraform Format
        run: terraform fmt -check
      
      - name: Terraform Unlock
        run: terraform force-unlock -force 1bcecd76-352b-7fd1-1dce-442e67ecb6c8 || echo "No locks to unlock"
      - name: Terraform Plan
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform
        if: env.IS_DESTROY != 'true'
        run: terraform apply -auto-approve

      - name: Generate kubeconfig and upload
        if: env.IS_DESTROY != 'true'
        run: |
          mkdir -p /tmp/kubeconfig
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example --kubeconfig /tmp/kubeconfig/config
        env:
          AWS_REGION: ap-southeast-2
        shell: bash

      - name: Upload kubeconfig as artifact
        uses: actions/upload-artifact@v4
        with:
          name: bastion-kubeconfig
          path: /tmp/kubeconfig/config

      - name: Upload deployment_key.pem as artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem

      - name: Install kubectl and check EKS status
        if: env.IS_DESTROY != 'true'
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes --request-timeout=60s
          echo "Checking component statuses..."
          kubectl get componentstatuses --request-timeout=60s
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system --request-timeout=60s

  
  destroy-infrastructure:
    if: ${{ github.event.inputs.destroy == 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install required tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example || true
      - name: Clean up AWS resources
        run: |
          echo "Cleaning up AWS resources..."
          
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query "Vpcs[0].VpcId" --output text || echo "")
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" == "None" ]; then
            echo "VPC not found, skipping cleanup"
            exit 0
          fi
          echo "VPC ID: $VPC_ID"
          
          # Delete load balancers within the specific VPC
          echo "Deleting AWS Load Balancers in VPC $VPC_ID..."
          aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | tr '\t' '\n' | while read LB_ARN; do
            if [ ! -z "$LB_ARN" ]; then
              echo "Deleting load balancer $LB_ARN..."
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            fi
          done
          
          # Wait for load balancers to be deleted
          echo "Waiting for AWS Load Balancers to be fully deleted..."
          sleep 120 # Increased wait time
          
          # Release Elastic IPs associated with the VPC (e.g., NAT Gateways if not managed by TF, or orphaned)
          # WARNING: This can be risky if not properly scoped. Terraform should manage its own EIPs.
          # This example attempts to find EIPs in the VPC that are not associated or associated with NATs.
          echo "Releasing unassociated or NAT Gateway Elastic IPs in VPC $VPC_ID (use with caution)..."
          aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text | tr '\t' '\n' | while read ALLOC_ID; do
            if [ ! -z "$ALLOC_ID" ]; then # Corrected variable from LB to ALLOC_ID
              # Check if EIP is associated with a NAT gateway or unassociated
              ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].AssociationId" --output text)
              # NETWORK_INTERFACE_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].NetworkInterfaceId" --output text) # Not used, can be removed
              IS_NAT_GW_EIP=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" "Name=nat-gateway-address.allocation-id,Values=$ALLOC_ID" --query "NatGateways" --output text)

              if [ "$ASSOCIATION_ID" == "None" ] || [ ! -z "$IS_NAT_GW_EIP" ] ; then
                echo "Attempting to release Elastic IP $ALLOC_ID..."
                # Disassociation might be needed if it's a NAT GW EIP that TF failed to delete
                if [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id "$ASSOCIATION_ID" || echo "Failed to disassociate $ALLOC_ID, might already be disassociated or managed elsewhere."
                    sleep 2
                fi
                aws ec2 release-address --allocation-id "$ALLOC_ID" || echo "Failed to release $ALLOC_ID, it might be in use or managed by Terraform."
              else
                echo "Skipping EIP $ALLOC_ID as it appears to be actively associated with a resource not identified as a NAT GW or unassociated."
              fi
            fi
          done
          
          # Get all subnets in the VPC
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text)
          echo "Subnets: $SUBNETS"

          # Delete network interfaces in each subnet
          for SUBNET in $SUBNETS; do
            echo "Cleaning up resources in subnet $SUBNET..."
            aws ec2 describe-network-interfaces --filters "Name=subnet-id,Values=$SUBNET" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text | tr '\t' '\n' | while read NI; do
              if [ ! -z "$NI" ]; then
                echo "Checking network interface $NI..."
                ATTACHMENT=$(aws ec2 describe-network-interfaces --network-interface-ids $NI --query "NetworkInterfaces[0].Attachment.AttachmentId" --output text)
                if [ "$ATTACHMENT" != "None" ] && [ ! -z "$ATTACHMENT" ]; then
                  echo "Detaching network interface $NI (attachment $ATTACHMENT)..."
                  aws ec2 detach-network-interface --attachment-id "$ATTACHMENT" --force
                  sleep 5
                fi
                echo "Deleting network interface $NI..."
                aws ec2 delete-network-interface --network-interface-id "$NI"
              fi
            done
          done
          
          # Wait for resources to be released
          echo "Waiting for resources to be released..."
          sleep 120 # Increased wait time

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Destroy
        working-directory: terraform
        run: terraform destroy -auto-approve

      
  deploy-argocd:
    if: github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true'
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Add Helm repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Get ACM certificate ARN from Terraform
        run: |
          # Get the certificate ARN from Terraform output
          cd terraform
          CERT_ARN=$(terraform output -raw acm_certificate_arn || echo "")
          
          if [ -z "$CERT_ARN" ]; then
            echo "Warning: Could not get certificate ARN from Terraform output"
            # Fallback to looking up the certificate
            CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            
            if [ -z "$CERT_ARN" ]; then
              echo "Error: No certificate found for *.bunnycloud.xyz"
              exit 1
            fi
          fi
          
          echo "Using certificate ARN: $CERT_ARN"
          echo "ACM_CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV

      - name: Install NGINX Ingress Controller with SSL
        run: |
          echo "Installing NGINX Ingress Controller for AWS with SSL..."
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${{ env.ACM_CERTIFICATE_ARN }}" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
            --set controller.service.targetPorts.https=http \
            --set controller.service.ports.https=443 \
            --set controller.replicaCount=2 \
            --version 4.10.1 \
            --wait
          
          echo "Waiting for ingress-nginx controller pod to be ready..."
          kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m

                echo "Installing ArgoCD..."
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --set server.service.type=LoadBalancer \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${{ env.ACM_CERTIFICATE_ARN }}" \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
            --set server.service.ports.https=443 \
            --version 6.11.1 \
            --wait
            
          echo "Checking ArgoCD pods status..."
          kubectl get pods -n argocd
          echo "Waiting for ArgoCD deployments to be ready..."
          kubectl wait --for=condition=Available deployment --all -n argocd --timeout=5m || true
          echo "ArgoCD installation completed."



      - name: Get ExternalDNS role ARN from Terraform
        run: |
          echo "Getting ExternalDNS role ARN from Terraform..."
          cd terraform
          ROLE_ARN=$(terraform output -raw external_dns_role_arn || echo "")
          
          if [ -z "$ROLE_ARN" ]; then
            echo "Warning: Could not get ExternalDNS role ARN from Terraform output"
            # Fallback to looking up the role
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
            ROLE_NAME="eks-external-dns-role"
            ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${ROLE_NAME}"
          fi
          
          echo "Using ExternalDNS role ARN: $ROLE_ARN"
          echo "EXTERNAL_DNS_ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV
          
          # Get hosted zone ID
          HOSTED_ZONE_ID=$(terraform output -raw route53_zone_id || aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          echo "Using hosted zone ID: $HOSTED_ZONE_ID"
          echo "HOSTED_ZONE_ID=$HOSTED_ZONE_ID" >> $GITHUB_ENV
          
          # Install ExternalDNS
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace external-dns --create-namespace \
            --set provider=aws \
            --set aws.zoneType=public \
            --set domainFilters[0]=bunnycloud.xyz \
            --set policy=upsert-only \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${EXTERNAL_DNS_ROLE_ARN}" \
            --set txtOwnerId=$HOSTED_ZONE_ID \
            --version 1.14.5 \
            --wait
          echo "ExternalDNS installed."
        continue-on-error: true  # Don't fail the workflow if ExternalDNS installation fails


      - name: Verify ArgoCD Installation and DNS
        run: |
          echo "Checking ArgoCD installation status..."
          kubectl get pods -n argocd
          
          echo "Checking ArgoCD service..."
          kubectl get svc -n argocd
          
          echo "Checking ArgoCD ingress..."
          kubectl get ingress -n argocd
          
          echo "Waiting for ArgoCD service to get an address..."
          ARGOCD_HOSTNAME=""
          for i in {1..15}; do
            ARGOCD_HOSTNAME=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ ! -z "$ARGOCD_HOSTNAME" ]; then
              echo "ArgoCD hostname found: $ARGOCD_HOSTNAME"
              break
            fi
            echo "Waiting for ArgoCD service to get an address... ($i/15)"
            sleep 20
          done
          
          if [ -z "$ARGOCD_HOSTNAME" ]; then
            echo "Warning: Could not find ArgoCD hostname after timeout."
            exit 0  # Don't fail the workflow
          fi
          
          echo "Creating DNS record for argocd.bunnycloud.xyz..."
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          
          # Create a JSON file for the Route53 change
          cat > argocd-dns-change.json << EOF
          {
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "argocd.bunnycloud.xyz",
                  "Type": "CNAME",
                  "TTL": 300,
                  "ResourceRecords": [
                    {
                      "Value": "$ARGOCD_HOSTNAME"
                    }
                  ]
                }
              }
            ]
          }
          EOF
          
          # Apply the DNS change
          aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://argocd-dns-change.json || true
          echo "DNS record created for argocd.bunnycloud.xyz pointing to $ARGOCD_HOSTNAME"
        continue-on-error: true  # Don't fail the workflow if verification fails

      # Security group rules are now managed by Terraform in eks_sg_rules.tf
      - name: Verify EKS security group rules
        run: |
          echo "Verifying EKS security group rules (now managed by Terraform)..."
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Cluster security group: $SG_ID"
          
          # Just verify the rules exist
          echo "Checking if security group allows required inbound traffic..."
          aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`443\` || FromPort==\`8080\`]" --output table
        continue-on-error: true  # Don't fail the workflow if verification fails

  generate-argo-apps:
    if: ${{ github.event.inputs.destroy != 'true' }}
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python and install Jinja2
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install jinja2-cli
      # Ensure yq is installed if you need it for other YAML processing
      # sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq
      - name: Detect new app folders
        id: detect
        run: |
          echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
          
      - name: Generate manifests
        run: |
          mkdir -p k8s/argocd/applications
          APPS_JSON='${{ steps.detect.outputs.apps }}'
          
          # Use repository variables or defaults
          DOCKER_USERNAME="${{ secrets.DOCKER_USERNAME || '1073286' }}"
          DEPLOYMENT_NAMESPACE="${{ vars.DEPLOYMENT_NAMESPACE || 'github-actions-example' }}"
          IMAGE_NAME="github-actions-example"  # Use a consistent image name
          
          # Get ACM certificate ARN
          ACM_CERT_ARN="${{ env.ACM_CERTIFICATE_ARN }}"
          if [ -z "$ACM_CERT_ARN" ]; then
            ACM_CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            echo "Retrieved ACM certificate ARN: $ACM_CERT_ARN"
          fi

          echo "$APPS_JSON" | jq -r '.[]' | while read app; do
            OUTFILE="k8s/argocd/applications/${app}.yaml"
            echo "Rendering $OUTFILE for app: $app"

            # Define image repository and tag
            IMAGE_REPOSITORY="${DOCKER_USERNAME}/${IMAGE_NAME}"  # Use the consistent image name
            IMAGE_TAG="${{ github.sha }}"
            TARGET_REVISION_BRANCH="${{ github.ref_name }}"
            
            # Override app_name to use github-actions-example
            APP_NAME="github-actions-example"

            jinja2 manifests/templates/argocd-app.yaml.j2 \
              -D app_name="$APP_NAME" -D chart_path="apps/$app/chart" \
              -D github_repository_url="${{ github.server_url }}/${{ github.repository }}" \
              -D target_revision="$TARGET_REVISION_BRANCH" \
              -D image_repository="$IMAGE_REPOSITORY" \
              -D image_tag="$IMAGE_TAG" \
              -D deployment_namespace="$DEPLOYMENT_NAMESPACE" \
              -D acm_certificate_arn="$ACM_CERT_ARN" \
              > "$OUTFILE"
            echo "Generated manifest for $app (using app_name=$APP_NAME):"
            cat "$OUTFILE"
          done


      - name: Commit generated ArgoCD Application manifests
        env:
          # Use PAT if GITHUB_TOKEN doesn't have push rights to main or if main is protected
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }} # Assuming this PAT has repo write access
        run: |
          # Commits directly to the branch the workflow is running on (e.g., 'main' after a PR merge).
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add k8s/argocd/applications/
          # Commit and push only if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "Generate/Update ArgoCD Application manifests for ${{ github.sha }}"
            git push origin HEAD:${{ github.ref_name }} # Push to the current branch
            echo "Committed and pushed ArgoCD Application manifests to ${{ github.ref_name }}."
          else
            echo "No changes to ArgoCD Application manifests to commit."
          fi

  lint-and-deploy-helm:
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && github.event.inputs.destroy != 'true'
    env:
      IS_DESTROY: ${{ github.event.inputs.destroy == 'true' }}
    needs: [docker, deploy-argocd] # Changed from generate-argo-apps to deploy-argocd
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Lint Helm charts
        run: |
          if [ -d "apps" ]; then  # Changed from "helm" to "apps"
            for chart_dir in apps/*/chart; do
              echo "Linting $chart_dir..."
              helm lint $chart_dir
            done
          else
            echo "No apps directory found, skipping Helm lint"
          fi

      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Rendering $chart_dir/chart..."
                # You might want to pass values files or set values here for a more realistic render
                helm template "$chart_dir/chart"
              fi
            done
          fi
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

      - name: Apply ArgoCD Application manifests
        run: |
          # Apply all generated application manifests
          if [ -d "k8s/argocd/applications" ]; then
            for app_manifest in k8s/argocd/applications/*.yaml; do
              if [ -f "$app_manifest" ]; then
                echo "Applying ArgoCD Application manifest: $app_manifest"
                kubectl apply -f "$app_manifest"
              fi
            done
          else
            echo "No ArgoCD application manifests found in k8s/argocd/applications."
          fi

      - name: Wait for Node.js app to be ready
        run: |
          APP_NAMESPACE="github-actions-example"
          APP_NAME="github-actions-example"  # Use the consistent app name

          echo "Creating namespace $APP_NAMESPACE if it doesn't exist..."
          kubectl create namespace $APP_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          echo "Waiting for ArgoCD to deploy the application..."
          # Wait for ArgoCD to create resources (up to 5 minutes)
          for i in {1..30}; do
            if kubectl get deployment -n $APP_NAMESPACE 2>/dev/null | grep -q "$APP_NAME"; then
              echo "Deployment found in namespace $APP_NAMESPACE"
              break
            fi
            echo "Waiting for deployment to be created... ($i/30)"
            sleep 10
            if [ $i -eq 30 ]; then
              echo "Warning: Deployment not found after timeout. ArgoCD may still be processing."
              echo "Checking ArgoCD application status:"
              kubectl get applications -n argocd
              echo "Continuing anyway..."
            fi
          done

          echo "Waiting for $APP_NAMESPACE pods to be ready..."
          kubectl get pods -n $APP_NAMESPACE
        continue-on-error: true  # Don't fail the workflow if the app isn't ready yet

      - name: Create DNS record for application
        run: |
          echo "Creating DNS record for github-actions-example.bunnycloud.xyz..."
          
          # Get the application's service hostname
          APP_HOSTNAME=""
          
          # First, check for LoadBalancer service
          for i in {1..15}; do
            APP_HOSTNAME=$(kubectl get svc -n github-actions-example -l app.kubernetes.io/instance=github-actions-example -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ -z "$APP_HOSTNAME" ]; then
              # Try to get hostname from ingress
              APP_HOSTNAME=$(kubectl get ingress -n github-actions-example -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            fi
            
            if [ ! -z "$APP_HOSTNAME" ]; then
              echo "Application hostname found: $APP_HOSTNAME"
              break
            fi
            echo "Waiting for application hostname... ($i/15)"
            sleep 20
          done
          
          # If no hostname found, exit
          if [ -z "$APP_HOSTNAME" ]; then
            echo "Warning: Could not find application hostname after timeout."
            kubectl get svc -n github-actions-example
            kubectl get ingress -n github-actions-example
            exit 0
          fi
          
          # Create DNS record
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          
          # Create a JSON file for the Route53 change
          cat > app-dns-change.json << EOF
          {
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "github-actions-example.bunnycloud.xyz",
                  "Type": "CNAME",
                  "TTL": 300,
                  "ResourceRecords": [
                    {
                      "Value": "$APP_HOSTNAME"
                    }
                  ]
                }
              }
            ]
          }
          EOF
              
          # Apply the DNS change
          aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://app-dns-change.json
          echo "DNS record created for github-actions-example.bunnycloud.xyz pointing to $APP_HOSTNAME"
          
          # Wait for DNS propagation
          echo "Waiting for DNS propagation..."
          for i in {1..10}; do
            if dig github-actions-example.bunnycloud.xyz +short | grep -q .; then
              echo "DNS record found for github-actions-example.bunnycloud.xyz"
              break
            fi
            echo "Waiting for DNS propagation... ($i/10)"
            sleep 30
          done
        continue-on-error: true  # Don't fail the workflow if DNS creation fails


      - name: Verify SSL/TLS for Node.js application
        run: |
          echo "Verifying SSL/TLS encryption for Node.js application..."
          
          # Get the application URL
          APP_URL="https://github-actions-example.bunnycloud.xyz"
          
          # Check if the URL is accessible with HTTPS
          echo "Checking if $APP_URL is accessible with HTTPS..."
          if curl -s -o /dev/null -w "%{http_code}" --connect-timeout 30 $APP_URL | grep -q "200\|301\|302"; then
            echo "✅ $APP_URL is accessible with HTTPS"
          else
            echo "❌ $APP_URL is not accessible with HTTPS"
            echo "Checking DNS record..."
            dig github-actions-example.bunnycloud.xyz
          fi
          
          # Verify SSL certificate details
          echo "Verifying SSL certificate details..."
          echo | openssl s_client -servername github-actions-example.bunnycloud.xyz -connect github-actions-example.bunnycloud.xyz:443 2>/dev/null | openssl x509 -noout -text | grep -A2 "Issuer:"
          
          # Check if certificate is from Amazon (ACM)
          if echo | openssl s_client -servername github-actions-example.bunnycloud.xyz -connect github-actions-example.bunnycloud.xyz:443 2>/dev/null | openssl x509 -noout -text | grep -q "Amazon"; then
            echo "✅ Certificate is issued by Amazon (ACM)"
          else
            echo "❌ Certificate is not issued by Amazon"
          fi
        continue-on-error: true  # Don't fail the workflow if verification fails
