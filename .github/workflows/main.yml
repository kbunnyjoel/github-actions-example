name: Nodejs CI with code coverage

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: 'Set to true to destroy the EKS cluster'
        required: false
        default: 'false'
      run_terraform:
        description: 'Run Terraform? (true/false)'
        required: false
        default: 'true'
  push:
    branches: [ main ]
    paths:
      - 'apps/**'
      - 'manifests/templates/**'
  pull_request:
    branches: [ main, 'feature/*' ]

env:
  APP_NAME: ${{ github.event.repository.name }}
  REGISTRY: docker.io/${{ secrets.DOCKER_USERNAME }}

jobs:
  build-and-test:
    permissions:
      actions: read # Default permission for GITHUB_TOKEN
      contents: read # To checkout the repository
      security-events: write # Required for CodeQL to upload SARIF results
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [20.x]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: List files in workspace
        run: ls -l

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests and collect coverage
        run: npm test

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: "/language:javascript"

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-node-${{ matrix.node-version }}
          path: coverage/

  docker:
    name: Build and push Docker image
    runs-on: ubuntu-latest
    permissions:
      packages: write
    needs: build-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GHCR login
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract short SHA
        id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - name: Container Registry push image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}
            ghcr.io/${{ github.repository_owner }}/${{ env.APP_NAME }}:${{ github.sha }}

      # Scan the image after it has been built and loaded/pushed
      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}'
          format: 'table'
          exit-code: '1' # Fail the build if vulnerabilities are found (adjust severity as needed)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'
          # Ensure the scanner checks the local Docker daemon
          # Trivy action usually does this by default if image is found locally.

      - name: Run Docker container in background
        run: docker run -d -p 3000:3000 --name live-app ${{ secrets.DOCKER_USERNAME }}/${{ env.APP_NAME }}:${{ github.sha }}

      - name: Wait for app to be ready
        run: |
          for i in {1..20}; do
            curl --fail http://localhost:3000/status && exit 0
            echo "Waiting for app to be ready ($i/20)..."
            sleep 5
          done

  terraform-eks:
    if: ${{ github.event.inputs.run_terraform == 'true' }}
    runs-on: ubuntu-latest
    needs: docker
    defaults:
      run:
        working-directory: terraform
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Run tfsec IaC scan
        uses: aquasecurity/tfsec-action@v1.0.0 # Use a specific version if preferred
        with:
          working_directory: terraform
          addtional_args: '--format json --output-file tfsec-report.json'
          soft_fail: true # Uncomment to see warnings without failing the build initially
      
      - name: Upload tfsec report
        uses: actions/upload-artifact@v4
        with:
          name: tfsec-report
          path: terraform/tfsec-report.json
      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Validate
        run: terraform validate
      - name: Terraform Format
        run: terraform fmt -check
      
      - name: Terraform Plan
        run: terraform plan

      - name: Terraform Apply
        working-directory: terraform
        if: github.event.inputs.destroy != 'true'
        run: terraform apply -auto-approve

      - name: Generate kubeconfig and upload
        if: github.event.inputs.destroy != 'true'
        run: |
          mkdir -p /tmp/kubeconfig
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example --kubeconfig /tmp/kubeconfig/config
        env:
          AWS_REGION: ap-southeast-2
        shell: bash

      - name: Upload kubeconfig as artifact
        uses: actions/upload-artifact@v4
        with:
          name: bastion-kubeconfig
          path: /tmp/kubeconfig/config

      - name: Upload deployment_key.pem as artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-key
          path: terraform/keys/deployment_key.pem

      - name: Install kubectl and check EKS status
        if: github.event.inputs.destroy != 'true'
        run: |
          set -e
          VERSION=v1.29.2 # Use a valid and more recent kubectl version
          echo "Installing kubectl version: $VERSION"
          curl -LO "https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/

          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

          echo "Checking Kubernetes nodes..."
          kubectl get nodes --request-timeout=60s
          echo "Checking component statuses..."
          kubectl get componentstatuses --request-timeout=60s
          echo "Listing kube-system pods..."
          kubectl get pods -n kube-system --request-timeout=60s

  
  destroy-infrastructure:
    if: github.event.inputs.destroy == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Install required tools
        run: |
          curl -LO "https://dl.k8s.io/release/v1.29.2/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example || true
      - name: Clean up Kubernetes resources
        run: |
          echo "Cleaning up Kubernetes resources (Ingresses, Services, Deployments, Namespaces)..."
          kubectl delete ingress --all --all-namespaces || true

          # Delete services of type LoadBalancer first and wait for AWS LBs to be de-provisioned
          echo "Deleting LoadBalancer services in argocd namespace..."
          kubectl delete service -n argocd --selector='service.kubernetes.io/load-balancer-cleanup' || kubectl delete service --all -n argocd --field-selector type=LoadBalancer || true
          echo "Deleting LoadBalancer services in ingress-nginx namespace..."
          kubectl delete service -n ingress-nginx --selector='service.kubernetes.io/load-balancer-cleanup' || kubectl delete service --all -n ingress-nginx --field-selector type=LoadBalancer || true
          
          echo "Waiting up to 5 minutes for AWS Load Balancers from Kubernetes services to be deleted..."
          # This is a best-effort wait; more sophisticated polling might be needed for very complex setups
          # The goal is to give AWS time to react to the k8s service deletions.
          sleep 300 

          kubectl delete deployment --all -n argocd || true
          kubectl delete deployment --all -n ingress-nginx || true
          # Delete other resources within namespaces before deleting namespaces
          kubectl delete all --all -n argocd || true
          kubectl delete all --all -n ingress-nginx || true
          kubectl delete namespace argocd || true
          kubectl delete namespace ingress-nginx || true

      - name: Clean up AWS resources
        run: |
          echo "Cleaning up AWS resources..."
          
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=eks-vpc" --query "Vpcs[0].VpcId" --output text || echo "")
          if [ -z "$VPC_ID" ] || [ "$VPC_ID" == "None" ]; then
            echo "VPC not found, skipping cleanup"
            exit 0
          fi
          echo "VPC ID: $VPC_ID"
          
          # Delete load balancers within the specific VPC
          echo "Deleting AWS Load Balancers in VPC $VPC_ID..."
          aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | tr '\t' '\n' | while read LB_ARN; do
            if [ ! -z "$LB_ARN" ]; then
              echo "Deleting load balancer $LB_ARN..."
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            fi
          done
          
          # Wait for load balancers to be deleted
          echo "Waiting for AWS Load Balancers to be fully deleted..."
          sleep 120 # Increased wait time
          
          # Release Elastic IPs associated with the VPC (e.g., NAT Gateways if not managed by TF, or orphaned)
          # WARNING: This can be risky if not properly scoped. Terraform should manage its own EIPs.
          # This example attempts to find EIPs in the VPC that are not associated or associated with NATs.
          echo "Releasing unassociated or NAT Gateway Elastic IPs in VPC $VPC_ID (use with caution)..."
          aws ec2 describe-addresses --filters "Name=domain,Values=vpc" --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text | tr '\t' '\n' | while read ALLOC_ID; do
            if [ ! -z "$ALLOC_ID" ]; then # Corrected variable from LB to ALLOC_ID
              # Check if EIP is associated with a NAT gateway or unassociated
              ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].AssociationId" --output text)
              # NETWORK_INTERFACE_ID=$(aws ec2 describe-addresses --allocation-ids "$ALLOC_ID" --query "Addresses[0].NetworkInterfaceId" --output text) # Not used, can be removed
              IS_NAT_GW_EIP=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" "Name=nat-gateway-address.allocation-id,Values=$ALLOC_ID" --query "NatGateways" --output text)

              if [ "$ASSOCIATION_ID" == "None" ] || [ ! -z "$IS_NAT_GW_EIP" ] ; then
                echo "Attempting to release Elastic IP $ALLOC_ID..."
                # Disassociation might be needed if it's a NAT GW EIP that TF failed to delete
                if [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id "$ASSOCIATION_ID" || echo "Failed to disassociate $ALLOC_ID, might already be disassociated or managed elsewhere."
                    sleep 2
                fi
                aws ec2 release-address --allocation-id "$ALLOC_ID" || echo "Failed to release $ALLOC_ID, it might be in use or managed by Terraform."
              else
                echo "Skipping EIP $ALLOC_ID as it appears to be actively associated with a resource not identified as a NAT GW or unassociated."
              fi
            fi
          done
          
          # Get all subnets in the VPC
          SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[*].SubnetId" --output text)
          echo "Subnets: $SUBNETS"

          # Delete network interfaces in each subnet
          for SUBNET in $SUBNETS; do
            echo "Cleaning up resources in subnet $SUBNET..."
            aws ec2 describe-network-interfaces --filters "Name=subnet-id,Values=$SUBNET" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text | tr '\t' '\n' | while read NI; do
              if [ ! -z "$NI" ]; then
                echo "Checking network interface $NI..."
                ATTACHMENT=$(aws ec2 describe-network-interfaces --network-interface-ids $NI --query "NetworkInterfaces[0].Attachment.AttachmentId" --output text)
                if [ "$ATTACHMENT" != "None" ] && [ ! -z "$ATTACHMENT" ]; then
                  echo "Detaching network interface $NI (attachment $ATTACHMENT)..."
                  aws ec2 detach-network-interface --attachment-id "$ATTACHMENT" --force
                  sleep 5
                fi
                echo "Deleting network interface $NI..."
                aws ec2 delete-network-interface --network-interface-id "$NI"
              fi
            done
          done
          
          # Wait for resources to be released
          echo "Waiting for resources to be released..."
          sleep 120 # Increased wait time

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.11.0" # Example: Use a more recent stable version

      - name: Terraform Init
        working-directory: terraform
        run: terraform init

      - name: Terraform Destroy
        working-directory: terraform
        run: terraform destroy -auto-approve

      
  deploy-argocd:
    if: github.event.inputs.run_terraform == 'true' && github.event.inputs.destroy != 'true'
    needs: terraform-eks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example
      - name: Wait for EKS API and nodes to be ready
        run: |
          echo "Waiting for Kubernetes API..."
          for i in {1..30}; do
            kubectl version && break || sleep 10
          done
          echo "Waiting for at least one node to be Ready..."
          for i in {1..30}; do
            kubectl get nodes | grep -q ' Ready ' && break || sleep 10
          done

      - name: Apply aws-auth ConfigMap for bastion access
        run: |
          echo "Applying aws-auth ConfigMap for bastion access..."
          kubectl apply -f k8s/argocd/aws-auth-patch.yaml
          echo "aws-auth ConfigMap applied"

      - name: Add Helm repositories
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

      - name: Get ACM certificate ARN from Terraform
        run: |
          # Get the certificate ARN from Terraform output
          cd terraform
          CERT_ARN=$(terraform output -raw acm_certificate_arn || echo "")
          
          if [ -z "$CERT_ARN" ]; then
            echo "Warning: Could not get certificate ARN from Terraform output"
            # Fallback to looking up the certificate
            CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?DomainName=='*.bunnycloud.xyz'].CertificateArn" --output text)
            
            if [ -z "$CERT_ARN" ]; then
              echo "Error: No certificate found for *.bunnycloud.xyz"
              exit 1
            fi
          fi
          
          echo "Using certificate ARN: $CERT_ARN"
          echo "ACM_CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV

      - name: Install NGINX Ingress Controller with SSL
        run: |
          echo "Installing NGINX Ingress Controller for AWS with SSL..."
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx --create-namespace \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${{ env.ACM_CERTIFICATE_ARN }}" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
            --set controller.service.targetPorts.https=http \
            --set controller.service.ports.https=443 \
            --set controller.replicaCount=2 \
            --version 4.10.1 \
            --wait
          
          echo "Waiting for ingress-nginx controller pod to be ready..."
          kubectl rollout status deployment ingress-nginx-controller -n ingress-nginx --timeout=5m

      - name: Install Argo CD
        run: |
          echo "Installing ArgoCD..."
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd --create-namespace \
            --set server.ingress.enabled=true \
            --set server.ingress.ingressClassName=nginx \
            --set server.ingress.hostname=argocd.bunnycloud.xyz \
            --set server.ingress.annotations."external-dns\.alpha\.kubernetes\.io/hostname"=argocd.bunnycloud.xyz. \
            --set server.ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
            --set server.ingress.annotations."nginx\.ingress\.kubernetes\.io/ssl-redirect"=true \
            --set server.ingress.annotations."nginx\.ingress\.kubernetes\.io/force-ssl-redirect"=true \
            --version 6.11.1 \
            --wait
          echo "Checking ArgoCD pods status..."
          kubectl get pods -n argocd
          echo "Waiting for ArgoCD deployments to be ready..."
          kubectl wait --for=condition=Available deployment --all -n argocd --timeout=5m || true
          echo "ArgoCD installation completed."


      # Removed duplicate "Verify and fix ArgoCD DNS" step.


      - name: Install ExternalDNS
        run: |
          echo "Installing ExternalDNS..."
          # Get AWS account ID from current credentials
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
          echo "Using AWS Account ID: $AWS_ACCOUNT_ID"
          
          # Create IAM role for ExternalDNS if it doesn't exist
          ROLE_NAME="eks-external-dns-role"
          if ! aws iam get-role --role-name $ROLE_NAME 2>/dev/null; then
            echo "Creating IAM role for ExternalDNS..."
            
            # Create trust policy document
            cat > trust-policy.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/$(aws eks describe-cluster --name github-actions-eks-example --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5)"
                },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "oidc.eks.ap-southeast-2.amazonaws.com/id/$(aws eks describe-cluster --name github-actions-eks-example --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5):sub": "system:serviceaccount:external-dns:external-dns"
                  }
                }
              }
            ]
          }
          EOF
            
            # Create policy document
            cat > policy.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": [
                  "route53:ChangeResourceRecordSets"
                ],
                "Resource": [
                  "arn:aws:route53:::hostedzone/*"
                ]
              },
              {
                "Effect": "Allow",
                "Action": [
                  "route53:ListHostedZones",
                  "route53:ListResourceRecordSets"
                ],
                "Resource": [
                  "*"
                ]
              }
            ]
          }
          EOF
            
            # Create role and attach policy
            aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://trust-policy.json
            aws iam put-role-policy --role-name $ROLE_NAME --policy-name "AllowExternalDNSUpdates" --policy-document file://policy.json
            
            echo "IAM role created: $ROLE_NAME"
          else
            echo "IAM role already exists: $ROLE_NAME"
          fi
          
          # Get the role ARN
          ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${ROLE_NAME}"
          echo "Using role ARN: $ROLE_ARN"
          
          # Get hosted zone ID
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='bunnycloud.xyz.'].Id" --output text | sed 's/\/hostedzone\///')
          echo "Using hosted zone ID: $HOSTED_ZONE_ID"
          
          # Install ExternalDNS
          helm upgrade --install external-dns external-dns/external-dns \
            --namespace external-dns --create-namespace \
            --set provider=aws \
            --set aws.zoneType=public \
            --set domainFilters[0]=bunnycloud.xyz \
            --set policy=upsert-only \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${ROLE_ARN}" \
            --set txtOwnerId=$HOSTED_ZONE_ID \
            --version 1.14.5 \
            --wait
          echo "ExternalDNS installed."
        continue-on-error: true  # Don't fail the workflow if ExternalDNS installation fails


      - name: Verify Ingress and DNS (ArgoCD)
        run: |
          echo "Waiting for ArgoCD Ingress or Service to get an address..."
          HOSTNAME=""
          for i in {1..30}; do
            # Check for Ingress
            INGRESS_HOSTNAME=$(kubectl get ingress -n argocd -l app.kubernetes.io/name=argocd-server -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            # If Ingress not found, check for LoadBalancer Service
            if [ -z "$INGRESS_HOSTNAME" ]; then
              INGRESS_HOSTNAME=$(kubectl get svc -n argocd argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            fi
            
            if [ ! -z "$INGRESS_HOSTNAME" ]; then
              echo "LoadBalancer hostname found: $INGRESS_HOSTNAME"
              break
            fi
            echo "Waiting for LoadBalancer... (attempt $i/30)"
            sleep 20
          done
          
          if [ -z "$INGRESS_HOSTNAME" ]; then
            echo "Error: ArgoCD LoadBalancer not found after timeout."
            echo "Checking Ingress resources:"
            kubectl get ingress -n argocd -o wide
            echo "Checking Service resources:"
            kubectl get svc -n argocd -o wide
            exit 1
          fi
          
          # Check ExternalDNS logs to see if it's working
          echo "Checking ExternalDNS logs..."
          kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=50 || true
          
          echo "Waiting for DNS record for argocd.bunnycloud.xyz to be created by ExternalDNS..."
          for i in {1..5}; do # Reduced wait time to 2.5 minutes
            if dig +short argocd.bunnycloud.xyz | grep -q .; then
              echo "DNS record found for argocd.bunnycloud.xyz"
              break
            fi
            echo "DNS not propagated yet for argocd.bunnycloud.xyz... ($i/5)"
            sleep 30
          done
          
          # Let ExternalDNS handle DNS records - just log status
          if ! dig +short argocd.bunnycloud.xyz | grep -q .; then
            echo "DNS record for argocd.bunnycloud.xyz not found yet. ExternalDNS should create it automatically."
            echo "Checking ExternalDNS logs for troubleshooting:"
            kubectl logs -n external-dns -l app.kubernetes.io/name=external-dns --tail=20 || true
            
            echo "Verifying ArgoCD ingress has proper annotations:"
            kubectl get ingress -n argocd -o yaml | grep -A5 annotations || true
            
            echo "DNS propagation may take some time. Continuing..."
          else
            echo "DNS record for argocd.bunnycloud.xyz found and propagated."
          fi
         
          # Final DNS check
          echo "Final DNS check:"
          dig argocd.bunnycloud.xyz || true
        continue-on-error: true  # Don't fail the workflow if DNS isn't ready



      # Security group rules are now managed by Terraform in eks_sg_rules.tf
      - name: Verify EKS security group rules
        run: |
          echo "Verifying EKS security group rules (now managed by Terraform)..."
          SG_ID=$(aws eks describe-cluster --name github-actions-eks-example --region ap-southeast-2 --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
          echo "Cluster security group: $SG_ID"
          
          # Just verify the rules exist
          echo "Checking if security group allows required inbound traffic..."
          aws ec2 describe-security-groups --group-ids $SG_ID --query "SecurityGroups[0].IpPermissions[?FromPort==\`443\` || FromPort==\`8080\`]" --output table
        continue-on-error: true  # Don't fail the workflow if verification fails

          
      # The 'Check domain nameserver delegation' and 'Check Route53 record' steps can remain for verification.
      # The 'Update Nodejs App DNS record' step is removed as ExternalDNS will handle it
      # based on annotations in your Node.js app's Ingress (defined in its Helm chart).

      # Removed duplicate "Verify and fix ArgoCD DNS" step.
    
      - name: Verify Node.js App DNS
        run: |
          echo "Waiting for Node.js app ingress to get an address..."
          NODEJS_HOSTNAME=""
          
          # First, check for Ingress
          for i in {1..10}; do
            NODEJS_HOSTNAME=$(kubectl get ingress -n github-actions-example -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            if [ ! -z "$NODEJS_HOSTNAME" ]; then
              echo "Node.js Ingress hostname found: $NODEJS_HOSTNAME"
              break
            fi
            echo "Waiting for Node.js Ingress... (attempt $i/10)"
            sleep 10
          done
          
          # If no Ingress found, check for Service
          if [ -z "$NODEJS_HOSTNAME" ]; then
            for i in {1..10}; do
              NODEJS_HOSTNAME=$(kubectl get svc -n github-actions-example -l app.kubernetes.io/name=nodejs-app -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
              if [ ! -z "$NODEJS_HOSTNAME" ]; then
                echo "Node.js Service hostname found: $NODEJS_HOSTNAME"
                break
              fi
              echo "Waiting for Node.js Service... (attempt $i/10)"
              sleep 10
            done
          fi
          
          # If still no hostname, use ArgoCD hostname as fallback
          if [ -z "$NODEJS_HOSTNAME" ]; then
            echo "No Node.js hostname found. Using ArgoCD hostname as fallback..."
            NODEJS_HOSTNAME=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            if [ -z "$NODEJS_HOSTNAME" ]; then
              echo "Error: Could not find any hostname to use"
              kubectl get ingress -n github-actions-example
              kubectl get svc -n github-actions-example
              exit 1
            fi
          fi
          
          echo "Using hostname for DNS record: $NODEJS_HOSTNAME"
          
          # Rest of the DNS record creation code...
  generate-argo-apps:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python and install Jinja2
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip3 install jinja2-cli
      # Ensure yq is installed if you need it for other YAML processing
      # sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq
      - name: Detect new app folders
        id: detect
        run: |
          echo "apps=$(ls apps/ | jq -R -s -c 'split("\n") | map(select(length > 0))')" >> $GITHUB_OUTPUT
      - name: Generate manifests
        run: |
          mkdir -p k8s/argocd/applications
          APPS_JSON='${{ steps.detect.outputs.apps }}'
          
          # Use repository variables or defaults
          DOCKER_USERNAME="${{ secrets.DOCKER_USERNAME || '1073286' }}"
          DEPLOYMENT_NAMESPACE="${{ vars.DEPLOYMENT_NAMESPACE || 'github-actions-example' }}"
          IMAGE_NAME="github-actions-example"  # Use a consistent image name

          echo "$APPS_JSON" | jq -r '.[]' | while read app; do
            OUTFILE="k8s/argocd/applications/${app}.yaml"
            echo "Rendering $OUTFILE for app: $app"

            # Define image repository and tag
            IMAGE_REPOSITORY="${DOCKER_USERNAME}/${IMAGE_NAME}"  # Use the consistent image name
            IMAGE_TAG="${{ github.sha }}"
            TARGET_REVISION_BRANCH="${{ github.ref_name }}"
            
            # Override app_name to use github-actions-example
            APP_NAME="github-actions-example"

            jinja2 manifests/templates/argocd-app.yaml.j2 \
              -D app_name="$APP_NAME" -D chart_path="apps/$app/chart" \
              -D github_repository_url="${{ github.server_url }}/${{ github.repository }}" \
              -D target_revision="$TARGET_REVISION_BRANCH" \
              -D image_repository="$IMAGE_REPOSITORY" \
              -D image_tag="$IMAGE_TAG" \
              -D deployment_namespace="$DEPLOYMENT_NAMESPACE" \
              > "$OUTFILE"
            echo "Generated manifest for $app (using app_name=$APP_NAME):"
            cat "$OUTFILE"
          done



      - name: Commit generated ArgoCD Application manifests
        env:
          # Use PAT if GITHUB_TOKEN doesn't have push rights to main or if main is protected
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }} # Assuming this PAT has repo write access
        run: |
          # Commits directly to the branch the workflow is running on (e.g., 'main' after a PR merge).
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add k8s/argocd/applications/
          # Commit and push only if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "Generate/Update ArgoCD Application manifests for ${{ github.sha }}"
            git push origin HEAD:${{ github.ref_name }} # Push to the current branch
            echo "Committed and pushed ArgoCD Application manifests to ${{ github.ref_name }}."
          else
            echo "No changes to ArgoCD Application manifests to commit."
          fi

  lint-and-deploy-helm:
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    needs: [docker, deploy-argocd] # Changed from generate-argo-apps to deploy-argocd
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      - name: Lint Helm charts
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Linting $chart_dir/chart..."
                helm lint "$chart_dir/chart"
              fi
            done
          fi
      - name: Template Helm charts (dry-run render)
        run: |
          if [ -d "apps" ]; then # Changed from "helm" to "apps"
            for chart_dir in apps/*; do # Iterate through app folders
              if [ -f "$chart_dir/chart/Chart.yaml" ]; then # Check for a Chart.yaml file
                echo "Rendering $chart_dir/chart..."
                # You might want to pass values files or set values here for a more realistic render
                helm template "$chart_dir/chart"
              fi
            done
          fi
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --region ap-southeast-2 --name github-actions-eks-example

      - name: Apply ArgoCD Application manifests
        run: |
          # Apply all generated application manifests
          if [ -d "k8s/argocd/applications" ]; then
            for app_manifest in k8s/argocd/applications/*.yaml; do
              if [ -f "$app_manifest" ]; then
                echo "Applying ArgoCD Application manifest: $app_manifest"
                kubectl apply -f "$app_manifest"
              fi
            done
          else
            echo "No ArgoCD application manifests found in k8s/argocd/applications."
          fi

      - name: Wait for Node.js app to be ready
        run: |
          APP_NAMESPACE="github-actions-example"
          APP_NAME="github-actions-example"  # Use the consistent app name

          echo "Creating namespace $APP_NAMESPACE if it doesn't exist..."
          kubectl create namespace $APP_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          echo "Waiting for ArgoCD to deploy the application..."
          # Wait for ArgoCD to create resources (up to 5 minutes)
          for i in {1..30}; do
            if kubectl get deployment -n $APP_NAMESPACE 2>/dev/null | grep -q "$APP_NAME"; then
              echo "Deployment found in namespace $APP_NAMESPACE"
              break
            fi
            echo "Waiting for deployment to be created... ($i/30)"
            sleep 10
            if [ $i -eq 30 ]; then
              echo "Warning: Deployment not found after timeout. ArgoCD may still be processing."
              echo "Checking ArgoCD application status:"
              kubectl get applications -n argocd
              echo "Continuing anyway..."
            fi
          done

          echo "Waiting for $APP_NAMESPACE pods to be ready..."
          kubectl get pods -n $APP_NAMESPACE
        continue-on-error: true  # Don't fail the workflow if the app isn't ready yet
